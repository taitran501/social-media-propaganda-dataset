import pandas as pd
import os
import re
import argparse
import unicodedata
import sys
from pathlib import Path
from datetime import datetime

# ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n import
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

# Th√™m th∆∞ m·ª•c cha v√†o path ƒë·ªÉ import config
from utils.file_utils import save_excel_file
import config

def parse_args():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Clean data for social media analysis')
    parser.add_argument('--version', '-v', help='Version to process (e.g., v1, v2)')
    
    # Th√™m c√°c t√πy ch·ªçn m·ªõi
    parser.add_argument('--source', '-s', choices=['output', 'merge', 'platform_split'],
                        default='output', help='Source directory to read data from')
    parser.add_argument('--input-file', '-i', 
                        help='Specific input file name (relative to source directory)')
    parser.add_argument('--output-file', '-o', 
                        help='Output file name (without directory)')
    parser.add_argument('--target', '-t', choices=['output', 'merge', 'platform_split', 'pre_summarize'],
                        default='output', help='Target directory to save results')
    
    return parser.parse_args()

def clean_data(version, source='output', input_filename=None, target='output', output_filename=None):
    """ƒê·ªçc file t·ª´ ngu·ªìn ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh v√† th·ª±c hi·ªán cleaning"""
    
    # L·∫•y ƒë∆∞·ªùng d·∫´n cho version
    paths = config.get_version_paths(version)
    
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ngu·ªìn d·ª±a tr√™n tham s·ªë source
    source_dir = paths.get(f"{source}_dir", paths['output_dir'])
    if source == 'platform_split':
        source_dir = paths['raw_dir'].parent / "platform_split"
    
    # X√°c ƒë·ªãnh file input
    if input_filename:
        input_file = source_dir / input_filename
    else:
        if source == 'output':
            input_file = source_dir / "merged_raw.xlsx"
        else:
            # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh file c·ª• th·ªÉ, t√¨m t·∫•t c·∫£ file Excel ƒë·ªÉ x·ª≠ l√Ω
            excel_files = list(source_dir.glob("*.xlsx"))
            if not excel_files:
                print(f"Kh√¥ng t√¨m th·∫•y file Excel n√†o trong {source_dir}")
                return None
            
            if len(excel_files) > 1:
                print(f"T√¨m th·∫•y nhi·ªÅu file Excel trong {source_dir}. Vui l√≤ng ch·ªâ ƒë·ªãnh file c·ª• th·ªÉ.")
                for i, file in enumerate(excel_files):
                    print(f"{i+1}. {file.name}")
                choice = input("Ch·ªçn file (s·ªë th·ª© t·ª± ho·∫∑c 'all' ƒë·ªÉ x·ª≠ l√Ω t·∫•t c·∫£): ")
                
                if choice.lower() == 'all':
                    # X·ª≠ l√Ω t·∫•t c·∫£ c√°c file
                    results = []
                    for file in excel_files:
                        print(f"\n----- ƒêang x·ª≠ l√Ω {file.name} -----")
                        result = clean_single_file(file, version, target, None)
                        if result is not None:
                            results.append(result)
                    return results
                else:
                    try:
                        idx = int(choice) - 1
                        input_file = excel_files[idx]
                    except:
                        print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá.")
                        return None
            else:
                input_file = excel_files[0]
    
    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n ƒë√≠ch v√† t√™n file output
    target_dir = paths.get(f"{target}_dir", paths['output_dir'])
    if target == 'platform_split':
        target_dir = paths['raw_dir'].parent / "platform_split"
    elif target == 'pre_summarize':
        target_dir = paths['output_dir'] / "pre_summarize"
    
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c ƒë√≠ch t·ªìn t·∫°i
    target_dir.mkdir(parents=True, exist_ok=True)
    
    # X√°c ƒë·ªãnh t√™n file output
    if not output_filename:
        base_name = input_file.stem
        if "_cleaned" not in base_name:
            base_name += "_cleaned"
        output_file = target_dir / f"{base_name}.xlsx"
    else:
        output_file = target_dir / output_filename
    
    # X·ª≠ l√Ω file
    return clean_single_file(input_file, version, target, output_file)
    
def filter_long_comments(df, max_words=300, save_filtered=True, output_dir=None):
    """
    L·ªçc b·ªè c√°c comment qu√° d√†i v√† th·ªëng k√™
    
    Args:
        df: DataFrame ch·ª©a d·ªØ li·ªáu
        max_words: S·ªë t·ª´ t·ªëi ƒëa cho ph√©p (default: 300)
        save_filtered: C√≥ l∆∞u file backup c√°c comment d√†i kh√¥ng
        output_dir: Th∆∞ m·ª•c ƒë·ªÉ l∆∞u backup file
    
    Returns:
        tuple: (filtered_df, removed_df)
    """
    print(f"\n{'='*50}")
    print(f"B∆Ø·ªöC L·ªåC COMMENT D√ÄI (>{max_words} t·ª´)")
    print(f"{'='*50}")
    
    if 'comment_raw' not in df.columns:
        print("Kh√¥ng t√¨m th·∫•y c·ªôt 'comment_raw', b·ªè qua b∆∞·ªõc l·ªçc comment d√†i")
        return df, pd.DataFrame()
    
    # T√≠nh s·ªë t·ª´ cho comment_raw
    print("ƒêang t√≠nh s·ªë t·ª´ cho t·∫•t c·∫£ comments...")
    df['word_count_temp'] = df['comment_raw'].apply(count_words)
    
    # Th·ªëng k√™ tr∆∞·ªõc khi l·ªçc
    total_comments = len(df)
    long_comments_mask = df['word_count_temp'] > max_words
    long_comments = df[long_comments_mask].copy()
    long_count = len(long_comments)
    
    print(f"\nüìä TH·ªêNG K√ä COMMENT D√ÄI:")
    print(f"  - T·ªïng comments: {total_comments:,}")
    print(f"  - Comments d√†i (>{max_words} t·ª´): {long_count:,} ({long_count/total_comments*100:.2f}%)")
    print(f"  - Comments gi·ªØ l·∫°i: {total_comments - long_count:,} ({(total_comments - long_count)/total_comments*100:.2f}%)")
    
    if long_count > 0:
        print(f"\nüîç PH√ÇN T√çCH COMMENTS D√ÄI:")
        print(f"  - T·ª´ ng·∫Øn nh·∫•t trong nh√≥m d√†i: {long_comments['word_count_temp'].min()}")
        print(f"  - T·ª´ d√†i nh·∫•t: {long_comments['word_count_temp'].max()}")
        print(f"  - Trung b√¨nh t·ª´ trong nh√≥m d√†i: {long_comments['word_count_temp'].mean():.1f}")
        print(f"  - Median t·ª´ trong nh√≥m d√†i: {long_comments['word_count_temp'].median():.1f}")
        
        # Hi·ªÉn th·ªã m·ªôt v√†i v√≠ d·ª• comment d√†i nh·∫•t
        print(f"\nüìù V√ç D·ª§ COMMENTS D√ÄI NH·∫§T (top 3):")
        top_long = long_comments.nlargest(3, 'word_count_temp')
        for i, (idx, row) in enumerate(top_long.iterrows()):
            word_count = row['word_count_temp']
            comment_preview = str(row['comment_raw'])[:150] + "..." if len(str(row['comment_raw'])) > 150 else str(row['comment_raw'])
            print(f"  {i+1}. {word_count} t·ª´: {comment_preview}")
        
        # L∆∞u c√°c comment d√†i ƒë·ªÉ ki·ªÉm tra
        if save_filtered and output_dir:
            try:
                long_comments_file = output_dir / f"long_comments_removed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                # Th√™m c·ªôt l√Ω do x√≥a
                long_comments['removal_reason'] = f'Comment longer than {max_words} words'
                long_comments[['comment_raw', 'word_count_temp', 'removal_reason']].to_excel(long_comments_file, index=False)
                print(f"  - ƒê√£ l∆∞u {long_count} comments d√†i v√†o: {long_comments_file}")
            except Exception as e:
                print(f"  - Kh√¥ng th·ªÉ l∆∞u file backup: {e}")
    
    # L·ªçc b·ªè comments d√†i
    filtered_df = df[~long_comments_mask].copy()
    
    # X√≥a c·ªôt word_count t·∫°m th·ªùi
    filtered_df = filtered_df.drop('word_count_temp', axis=1)
    long_comments = long_comments.drop('word_count_temp', axis=1) if long_count > 0 else pd.DataFrame()
    
    print(f"\n‚úÖ ƒê√É L·ªåC: C√≤n l·∫°i {len(filtered_df):,} comments ƒë·ªÉ x·ª≠ l√Ω")
    
    return filtered_df, long_comments

def balance_comments_advanced(df, max_comments_per_post=1000):
    """
    C√¢n b·∫±ng s·ªë l∆∞·ª£ng comment cho m·ªói b√†i post v·ªõi logic m·ªõi:
    1. T√¨m comments c√≥ t·ª´ kh√≥a nh·∫°y c·∫£m
    2. Cho ph√©p user ch·ªçn s·ªë l∆∞·ª£ng random t·ª´ comments c√≥ t·ª´ kh√≥a
    3. Random sampling c√°c comment c√≤n l·∫°i ƒë·ªÉ ƒë·∫°t max_comments_per_post
    """
    print(f"ƒêang c√¢n b·∫±ng s·ªë l∆∞·ª£ng comment (ch·ªâ x·ª≠ l√Ω posts c√≥ > {max_comments_per_post} comments)...")
    
    # Ki·ªÉm tra xem DataFrame c√≥ c√°c c·ªôt c·∫ßn thi·∫øt kh√¥ng
    required_cols = ['post_id', 'comment_raw']
    if not all(col in df.columns for col in required_cols):
        print("Kh√¥ng t√¨m th·∫•y c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ c√¢n b·∫±ng comment")
        return df, None
    
    # DANH S√ÅCH T·ª™ KH√ìA NH·∫†Y C·∫¢M - C·∫¨P NH·∫¨T THEO Y√äU C·∫¶U
    preserve_keywords = [
        "ph·∫£n ƒë·ªông", "ph·∫£n qu·ªëc", "ph·∫£n b·ªôi", "ƒë·∫£ng c∆∞·ªõp", 
        "ba que", "vi·ªát c·ªông", "b√≤ ƒë·ªè", "t√†u c·ªông", "ƒë·ªì ƒëƒ©",
        "c·ªông s·∫£n", "c·ªông ph·ªâ", "x·ª© v·∫πm", "barwhere",
        "ƒë·ªôc t√†i", "ƒë√†n √°p", "nh√¢n quy·ªÅn",
        "xhcn", "dcs", "dcsvn", "vnch", "H·ªì T·∫∑c", "hochochet", "redbull",
        "vndcch", "cs", "csvn", "vn", "b·ªçn ch·ªát", "t√†u kh·ª±a", "v+", "+san"
    ]
    
    # T·∫°o pattern ƒë·ªÉ t√¨m t·ª´ kh√≥a nh·∫°y c·∫£m
    pattern = '|'.join([re.escape(keyword) for keyword in preserve_keywords])
    
    # Th√™m c·ªôt ƒë√°nh d·∫•u comment c√≥ t·ª´ kh√≥a nh·∫°y c·∫£m
    df['has_preserve_keywords'] = df['comment_raw'].astype(str).str.contains(
        pattern, na=False, regex=True, case=False
    )
    
    # Th·ªëng k√™ tr∆∞·ªõc khi c√¢n b·∫±ng
    post_counts = df['post_id'].value_counts()
    total_comments = len(df)
    posts_over_limit = sum(post_counts > max_comments_per_post)
    total_preserve_keywords = df['has_preserve_keywords'].sum()
    
    print(f"T·ªïng s·ªë comment ban ƒë·∫ßu: {total_comments}")
    print(f"Comments c√≥ t·ª´ kh√≥a nh·∫°y c·∫£m: {total_preserve_keywords} ({total_preserve_keywords/total_comments*100:.1f}%)")
    print(f"S·ªë b√†i post c√≥ > {max_comments_per_post} comment: {posts_over_limit}")
    
    if posts_over_limit == 0:
        print("Kh√¥ng c√≥ b√†i post n√†o v∆∞·ª£t qu√° gi·ªõi h·∫°n comment, gi·ªØ nguy√™n dataset")
        df = df.drop('has_preserve_keywords', axis=1)
        return df, None
    
    # H·ªéI USER V·ªÄ CHI·∫æN L∆Ø·ª¢C L·∫§Y COMMENTS C√ì T·ª™ KH√ìA
    print(f"\nüéØ CHI·∫æN L∆Ø·ª¢C L·∫§Y COMMENTS C√ì T·ª™ KH√ìA NH·∫†Y C·∫¢M:")
    print(f"  1. L·∫•y t·∫•t c·∫£ comments c√≥ t·ª´ kh√≥a (nh∆∞ tr∆∞·ªõc)")
    print(f"  2. Ch·ªâ l·∫•y m·ªôt s·ªë l∆∞·ª£ng random t·ª´ comments c√≥ t·ª´ kh√≥a")
    
    strategy_choice = input("Ch·ªçn chi·∫øn l∆∞·ª£c (1 ho·∫∑c 2): ").strip()
    
    max_priority_comments = None
    if strategy_choice == "2":
        max_priority_input = input("Nh·∫≠p s·ªë l∆∞·ª£ng t·ªëi ƒëa comments c√≥ t·ª´ kh√≥a mu·ªën gi·ªØ m·ªói post (VD: 50): ").strip()
        try:
            max_priority_comments = int(max_priority_input)
            max_priority_comments = max(1, max_priority_comments)  # T·ªëi thi·ªÉu 1
            print(f"S·∫Ω ch·ªâ l·∫•y t·ªëi ƒëa {max_priority_comments} comments c√≥ t·ª´ kh√≥a m·ªói post")
        except ValueError:
            print("Gi√° tr·ªã kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c 1 (l·∫•y t·∫•t c·∫£)")
            strategy_choice = "1"
    
    # X·ª≠ l√Ω t·ª´ng post ri√™ng bi·ªát
    balanced_dfs = []
    removed_comments_df = []
    total_removed = 0
    posts_processed = 0
    posts_unchanged = 0
    
    for post_id, post_df in df.groupby('post_id'):
        comment_count = len(post_df)
        
        # CH·ªà X·ª¨ L√ù C√ÅC POST C√ì > max_comments_per_post COMMENTS
        if comment_count <= max_comments_per_post:
            balanced_dfs.append(post_df)
            posts_unchanged += 1
            continue
        
        posts_processed += 1
        
        # Ph√¢n lo·∫°i comments
        priority_comments = post_df[post_df['has_preserve_keywords']].copy()
        normal_comments = post_df[~post_df['has_preserve_keywords']].copy()
        
        priority_count = len(priority_comments)
        normal_count = len(normal_comments)
        
        print(f"  Post {post_id}: {comment_count} comments ({priority_count} c√≥ t·ª´ kh√≥a + {normal_count} th∆∞·ªùng) ‚Üí c·∫ßn gi·∫£m xu·ªëng {max_comments_per_post}")
        
        # X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng priority comments s·∫Ω gi·ªØ
        if strategy_choice == "2" and max_priority_comments and priority_count > max_priority_comments:
            # Random select t·ª´ priority comments
            selected_priority = priority_comments.sample(max_priority_comments, random_state=42)
            removed_priority = priority_comments.drop(selected_priority.index)
            priority_count_keep = max_priority_comments
            print(f"    ‚Üí Ch·ªâ gi·ªØ {max_priority_comments}/{priority_count} comments c√≥ t·ª´ kh√≥a (random)")
        else:
            # Gi·ªØ t·∫•t c·∫£ priority comments
            selected_priority = priority_comments.copy()
            removed_priority = pd.DataFrame()
            priority_count_keep = priority_count
            print(f"    ‚Üí Gi·ªØ t·∫•t c·∫£ {priority_count} comments c√≥ t·ª´ kh√≥a")
        
        # T√≠nh to√°n remaining slots cho normal comments
        remaining_slots = max_comments_per_post - priority_count_keep
        
        if remaining_slots <= 0:
            # Kh√¥ng c√≤n ch·ªó cho normal comments
            kept_comments = selected_priority
            removed_normal = normal_comments
            print(f"    ‚Üí Kh√¥ng c√≤n ch·ªó cho comments th∆∞·ªùng, lo·∫°i b·ªè t·∫•t c·∫£ {normal_count} comments th∆∞·ªùng")
        elif remaining_slots >= normal_count:
            # ƒê·ªß ch·ªó cho t·∫•t c·∫£ normal comments
            kept_comments = pd.concat([selected_priority, normal_comments])
            removed_normal = pd.DataFrame()
            print(f"    ‚Üí ƒê·ªß ch·ªó cho t·∫•t c·∫£ {normal_count} comments th∆∞·ªùng")
        else:
            # Ph·∫£i random normal comments
            kept_normal = normal_comments.sample(remaining_slots, random_state=42)
            kept_comments = pd.concat([selected_priority, kept_normal])
            removed_normal = normal_comments.drop(kept_normal.index)
            print(f"    ‚Üí Gi·ªØ {remaining_slots}/{normal_count} comments th∆∞·ªùng (random)")
        
        # Combine removed comments
        removed_comments = pd.concat([removed_priority, removed_normal]) if len(removed_priority) > 0 or len(removed_normal) > 0 else pd.DataFrame()
        
        # Th√™m l√Ω do x√≥a cho removed comments
        if len(removed_comments) > 0:
            removed_comments = removed_comments.copy()
            removed_comments['post_id_removed'] = post_id
            removed_comments_df.append(removed_comments)
        
        balanced_dfs.append(kept_comments)
        total_removed += len(removed_comments)
        
        print(f"    ‚Üí K·∫øt qu·∫£: Gi·ªØ {len(kept_comments)}, lo·∫°i b·ªè {len(removed_comments)} comments")
    
    # K·∫øt h·ª£p t·∫•t c·∫£ l·∫°i
    balanced_df = pd.concat(balanced_dfs, ignore_index=True)
    balanced_df = balanced_df.drop('has_preserve_keywords', axis=1)
    
    # T·∫°o DataFrame cho removed comments
    removed_df_combined = None
    if removed_comments_df:
        removed_df_combined = pd.concat(removed_comments_df, ignore_index=True)
        removed_df_combined = removed_df_combined.drop('has_preserve_keywords', axis=1)
    
    # Th·ªëng k√™ sau khi c√¢n b·∫±ng
    final_post_counts = balanced_df['post_id'].value_counts()
    print(f"\nK·∫øt qu·∫£ c√¢n b·∫±ng:")
    print(f"  - S·ªë posts ƒë∆∞·ª£c x·ª≠ l√Ω: {posts_processed}")
    print(f"  - S·ªë posts gi·ªØ nguy√™n (‚â§{max_comments_per_post} comments): {posts_unchanged}")
    print(f"  - ƒê√£ lo·∫°i b·ªè {total_removed} comments ({total_removed/total_comments*100:.1f}%)")
    print(f"  - Dataset m·ªõi: {len(balanced_df)} comments")
    print(f"  - Trung b√¨nh comments/post: {final_post_counts.mean():.1f}")
    print(f"  - Max comments/post: {final_post_counts.max()}")
    print(f"  - Min comments/post: {final_post_counts.min()}")
    
    return balanced_df, removed_df_combined

def clean_single_file(input_file, version, target, output_file=None):
    """X·ª≠ l√Ω m·ªôt file ƒë∆°n l·∫ª"""
    print(f"Version: {version}")
    print(f"Input file: {input_file}")
    
    if output_file is None:
        paths = config.get_version_paths(version)
        target_dir = paths.get(f"{target}_dir", paths['output_dir'])
        if target == 'platform_split':
            target_dir = paths['raw_dir'].parent / "platform_split"
        elif target == 'pre_summarize':
            target_dir = paths['output_dir'] / "pre_summarize"
            
        # ƒê·∫£m b·∫£o th∆∞ m·ª•c ƒë√≠ch t·ªìn t·∫°i
        target_dir.mkdir(parents=True, exist_ok=True)
        
        base_name = input_file.stem
        if "_cleaned" not in base_name:
            base_name += "_cleaned"
        output_file = target_dir / f"{base_name}.xlsx"
    
    print(f"Output file: {output_file}")
    
    # Ki·ªÉm tra file input c√≥ t·ªìn t·∫°i kh√¥ng
    if not input_file.exists():
        print(f"L·ªói: Kh√¥ng t√¨m th·∫•y file input: {input_file}")
        return None
    
    print(f"ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´: {input_file}")
    
    try:
        # ƒê·ªçc file Excel
        df = pd.read_excel(input_file)
        print(f"ƒê√£ ƒë·ªçc {len(df)} d√≤ng d·ªØ li·ªáu")
        
        initial_rows = len(df)
        all_removed_records = []  # List ƒë·ªÉ l∆∞u t·∫•t c·∫£ records b·ªã x√≥a
        
        # ===== B∆Ø·ªöC 1: L·ªåC COMMENT D√ÄI TR∆Ø·ªöC =====
        print("\n" + "="*60)
        print("B∆Ø·ªöC 1: L·ªåC COMMENT QU√Å D√ÄI")
        print("="*60)
        
        # H·ªèi ng∆∞·ªùi d√πng v·ªÅ ng∆∞·ª°ng t·ª´
        max_words_input = input(f"Nh·∫≠p s·ªë t·ª´ t·ªëi ƒëa cho comment (m·∫∑c ƒë·ªãnh 300, Enter ƒë·ªÉ b·ªè qua): ").strip()
        
        if max_words_input.lower() in ['', 'skip', 'b·ªè qua']:
            print("B·ªè qua b∆∞·ªõc l·ªçc comment d√†i")
            long_removed_df = pd.DataFrame()
        else:
            try:
                max_words = int(max_words_input) if max_words_input else 300
                max_words = max(50, max_words)  # T·ªëi thi·ªÉu 50 t·ª´
                print(f"S·ª≠ d·ª•ng ng∆∞·ª°ng: {max_words} t·ª´")
                
                df, long_removed_df = filter_long_comments(
                    df, 
                    max_words=max_words, 
                    save_filtered=True,
                    output_dir=output_file.parent
                )
                
                if len(long_removed_df) > 0:
                    all_removed_records.append(long_removed_df)
                    
            except ValueError:
                print("Gi√° tr·ªã kh√¥ng h·ª£p l·ªá, b·ªè qua b∆∞·ªõc l·ªçc comment d√†i")
                long_removed_df = pd.DataFrame()
        
        # ===== B∆Ø·ªöC 2: CLEANING CONTENT =====
        print("\n" + "="*60)
        print("B∆Ø·ªöC 2: CLEANING N·ªòI DUNG COMMENT")
        print("="*60)
        
        # CH·∫†Y CH·ªà CLEANING CHO comment_raw (KH√îNG CLEAN post_raw)
        print("ƒêang th·ª±c hi·ªán cleaning cho comment_raw...")
        
        if 'comment_raw' in df.columns:
            print("  - Cleaning c·ªôt comment_raw...")
            df['comment_raw'] = df['comment_raw'].apply(minimal_clean)
        else:
            print("  - Kh√¥ng t√¨m th·∫•y c·ªôt comment_raw")
            return None
        
        # ===== B∆Ø·ªöC 3: L·ªåC COMMENT NG·∫ÆN =====
        print("\n" + "="*60)
        print("B∆Ø·ªöC 3: L·ªåC COMMENT QU√Å NG·∫ÆN")
        print("="*60)
        
        # L·ªçc c√°c d√≤ng c√≥ comment_raw c√≥ s·ªë t·ª´ <= 3
        print("L·ªçc c√°c d√≤ng comment c√≥ s·ªë t·ª´ <= 3...")
        rows_before = len(df)
        
        # T·∫°o list c√°c index c·∫ßn gi·ªØ l·∫°i
        keep_indices = []
        removed_indices = []  # L∆∞u c√°c index b·ªã x√≥a
        
        for idx, row in df.iterrows():
            comment = row.get('comment_raw', '')
            if isinstance(comment, str):
                # ƒê·∫øm s·ªë t·ª´ trong comment sau khi l√†m s·∫°ch
                word_count = count_words(comment)
                if word_count > 3 or is_special_pattern(comment):
                    keep_indices.append(idx)
                else:
                    removed_indices.append(idx)
            else:
                removed_indices.append(idx)
        
        # T·∫°o DataFrame ch·ª©a c√°c record b·ªã x√≥a ƒë·ªÉ backup
        short_removed_df = df.loc[removed_indices].copy() if removed_indices else pd.DataFrame()
        
        # Th√™m th√¥ng tin chi ti·∫øt v·ªÅ l√Ω do x√≥a
        if len(short_removed_df) > 0:
            short_removed_df['removal_reason'] = short_removed_df.apply(
                lambda row: get_removal_reason(row.get('comment_raw', '')), axis=1
            )
            all_removed_records.append(short_removed_df)
        
        # L·ªçc DataFrame theo indices
        df = df.loc[keep_indices] if keep_indices else pd.DataFrame()
        
        # Reset index sau khi l·ªçc
        df = df.reset_index(drop=True)
        
        filtered_rows = rows_before - len(df)
        print(f"ƒê√£ l·ªçc b·ªè {filtered_rows} d√≤ng c√≥ s·ªë t·ª´ <= 3 ({filtered_rows/rows_before*100:.1f}%)")
        
        # ===== B∆Ø·ªöC 4: C√ÇN B·∫∞NG COMMENTS CHO M·ªñI POST =====
        print("\n" + "="*60)
        print("B∆Ø·ªöC 4: C√ÇN B·∫∞NG COMMENTS CHO M·ªñI POST")
        print("="*60)
        
        # Ki·ªÉm tra xem c√≥ c·ªôt post_id kh√¥ng
        if 'post_id' in df.columns:
            # H·ªèi ng∆∞·ªùi d√πng v·ªÅ gi·ªõi h·∫°n comments per post
            max_comments_input = input(f"Nh·∫≠p s·ªë comment t·ªëi ƒëa m·ªói post (m·∫∑c ƒë·ªãnh 1000, Enter ƒë·ªÉ b·ªè qua): ").strip()
            
            if max_comments_input.lower() in ['', 'skip', 'b·ªè qua']:
                print("B·ªè qua b∆∞·ªõc c√¢n b·∫±ng comments")
                balance_removed_df = pd.DataFrame()
            else:
                try:
                    max_comments_per_post = int(max_comments_input) if max_comments_input else 1000
                    max_comments_per_post = max(100, max_comments_per_post)  # T·ªëi thi·ªÉu 100
                    
                    df, balance_removed_df = balance_comments_advanced(df, max_comments_per_post=max_comments_per_post)
                    
                    # Th√™m balance_removed_df v√†o removed_df t·ªïng
                    if balance_removed_df is not None and len(balance_removed_df) > 0:
                        balance_removed_df['removal_reason'] = f'Balanced - exceeded max {max_comments_per_post} comments per post'
                        all_removed_records.append(balance_removed_df)
                        
                except ValueError:
                    print("Gi√° tr·ªã kh√¥ng h·ª£p l·ªá, b·ªè qua b∆∞·ªõc c√¢n b·∫±ng comments")
                    balance_removed_df = pd.DataFrame()
        else:
            print("Kh√¥ng t√¨m th·∫•y c·ªôt post_id, b·ªè qua b∆∞·ªõc c√¢n b·∫±ng comments")
            balance_removed_df = pd.DataFrame()
        
        # ===== T·ªîNG H·ª¢P V√Ä L∆ØU K·∫æT QU·∫¢ =====
        print("\n" + "="*60)
        print("T·ªîNG H·ª¢P K·∫æT QU·∫¢")
        print("="*60)
        
        # T·∫°o file backup cho t·∫•t c·∫£ records b·ªã x√≥a
        if all_removed_records:
            combined_removed_df = pd.concat(all_removed_records, ignore_index=True)
            
            if len(combined_removed_df) > 0:
                backup_file = output_file.parent / f"{output_file.stem}_removed_records.xlsx"
                print(f"\nƒêang t·∫°o file backup cho {len(combined_removed_df)} record b·ªã x√≥a: {backup_file}")
                save_excel_file(combined_removed_df, backup_file)
                print(f"File backup ƒë√£ ƒë∆∞·ª£c l∆∞u: {backup_file}")
                
                # In th·ªëng k√™ chi ti·∫øt v·ªÅ l√Ω do x√≥a
                print("\nTh·ªëng k√™ l√Ω do x√≥a:")
                reason_counts = combined_removed_df['removal_reason'].value_counts()
                for reason, count in reason_counts.items():
                    print(f"  - {reason}: {count:,} records")
        
        # L∆∞u k·∫øt qu·∫£
        print(f"\nƒêang l∆∞u k·∫øt qu·∫£ v√†o: {output_file}")
        save_excel_file(df, output_file)
        
        print(f"\n‚úÖ ƒê√É HO√ÄN TH√ÄNH CLEANING!")
        print(f"T·ªïng s·ªë d√≤ng ban ƒë·∫ßu: {initial_rows:,}")
        print(f"T·ªïng s·ªë d√≤ng sau khi cleaning: {len(df):,}")
        total_removed = sum(len(removed_df) for removed_df in all_removed_records) if all_removed_records else 0
        print(f"T·ªïng s·ªë record b·ªã x√≥a: {total_removed:,} ({total_removed/initial_rows*100:.1f}%)")
        
        # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng d·ªØ li·ªáu theo platform
        if 'platform' in df.columns:
            platform_counts = df['platform'].value_counts()
            print("\nPh√¢n b·ªë theo platform sau cleaning:")
            for platform, count in platform_counts.items():
                print(f"  - {platform}: {count:,} d√≤ng ({count/len(df)*100:.1f}%)")
        
        # Hi·ªÉn th·ªã th·ªëng k√™ comments per post - S·ª¨A L·∫†I LOGIC
        if 'post_id' in df.columns and 'post_raw' in df.columns:
            # Th·ªëng k√™ theo unique post_raw (s·ªë b√†i post th·ª±c t·∫ø)
            unique_posts = df['post_raw'].nunique()
            print(f"\nTh·ªëng k√™ posts sau cleaning:")
            print(f"  - S·ªë l∆∞·ª£ng unique posts (post_raw): {unique_posts:,}")
            print(f"  - T·ªïng s·ªë comments: {len(df):,}")
            print(f"  - Trung b√¨nh comments/post: {len(df)/unique_posts:.1f}")
            
            # ƒê·∫øm comments cho m·ªói unique post_raw
            post_raw_counts = df['post_raw'].value_counts()
            print(f"  - Median comments/post: {post_raw_counts.median():.1f}")
            print(f"  - Max comments/post: {post_raw_counts.max():,}")
            print(f"  - Min comments/post: {post_raw_counts.min():,}")
            
            # Ki·ªÉm tra consistency gi·ªØa post_raw v√† post_id
            unique_post_ids = df['post_id'].nunique()
            if unique_posts != unique_post_ids:
                print(f"  ‚ö†Ô∏è C√≥ s·ª± kh√°c bi·ªát gi·ªØa unique post_raw ({unique_posts}) v√† post_id ({unique_post_ids})!")
            
            # Ph√¢n b·ªë s·ªë comment per unique post_raw
            print(f"\nPh√¢n b·ªë s·ªë comments per unique post:")
            ranges = [(1, 10), (11, 50), (51, 100), (101, 500), (501, 1000), (1001, float('inf'))]
            for min_val, max_val in ranges:
                if max_val == float('inf'):
                    count = sum((post_raw_counts > min_val))
                    label = f">{min_val}"
                else:
                    count = sum((post_raw_counts >= min_val) & (post_raw_counts <= max_val))
                    label = f"{min_val}-{max_val}"
                percentage = count / len(post_raw_counts) * 100
                print(f"  - {label} comments: {count:,} posts ({percentage:.1f}%)")
        
        elif 'post_raw' in df.columns:
            # N·∫øu ch·ªâ c√≥ post_raw m√† kh√¥ng c√≥ post_id
            unique_posts = df['post_raw'].nunique()
            print(f"\nTh·ªëng k√™ posts sau cleaning:")
            print(f"  - T·ªïng s·ªë unique posts (post_raw): {unique_posts:,}")
            print(f"  - T·ªïng s·ªë comments: {len(df):,}")
            print(f"  - Trung b√¨nh comments/post: {len(df)/unique_posts:.1f}")
            
            # ƒê·∫øm comments cho m·ªói unique post_raw
            post_raw_counts = df['post_raw'].value_counts()
            print(f"  - Median comments/post: {post_raw_counts.median():.1f}")
            print(f"  - Max comments/post: {post_raw_counts.max():,}")
            print(f"  - Min comments/post: {post_raw_counts.min():,}")
        
        return df
        
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω file: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def count_words(text):
    """ƒê·∫øm s·ªë t·ª´ trong vƒÉn b·∫£n"""
    if not isinstance(text, str):
        return 0
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† t√°ch th√†nh c√°c t·ª´
    words = text.strip().split()
    
    # L·ªçc b·ªè c√°c "t·ª´" ch·ªâ ch·ª©a d·∫•u c√¢u ho·∫∑c k√Ω t·ª± ƒë·∫∑c bi·ªát
    meaningful_words = []
    for word in words:
        # Ki·ªÉm tra xem t·ª´ c√≥ ch·ª©a √≠t nh·∫•t m·ªôt ch·ªØ c√°i ho·∫∑c s·ªë kh√¥ng
        if re.search(r'[a-zA-Z√Ä-·ªπ0-9]', word):
            meaningful_words.append(word)
    
    return len(meaningful_words)

def get_removal_reason(comment):
    """X√°c ƒë·ªãnh l√Ω do x√≥a record"""
    if not isinstance(comment, str):
        return "Empty or invalid comment"
    
    word_count = count_words(comment)
    
    if word_count == 0:
        return "Empty comment after cleaning"
    elif word_count == 1:
        return "Single word comment"
    elif word_count == 2:
        return "Two words comment"
    elif word_count == 3:
        return "Three words comment"
    else:
        return "Other reason"

def is_special_pattern(text):
    """Check if text contains special patterns that should be kept despite word count"""
    if not isinstance(text, str):
        return False
    
    # Gi·ªØ l·∫°i c√°c pattern ƒë·∫∑c bi·ªát v√† t·ª´ kh√≥a c√≥ √Ω nghƒ©a
    special_patterns = [
        # C√°c k√Ω hi·ªáu ƒë·∫∑c bi·ªát
        "///", "3/", "3///", "3//", "3|||",
        
        # C√°c t·ª´ vi·∫øt t·∫Øt ch√≠nh tr·ªã quan tr·ªçng
        "cs", "csvn", "dcsvn", "xhcn", "dcs", "vc", "vnch", "vndcch", 
        "redbull", "b√≤ ƒë·ªè", "ba que", "ba s·ªçc", "vi·ªát c·ªông", "vn c·ªông",
        "ph·∫£n ƒë·ªông", "ƒë·∫£ng tr·ªã", "barwhere", "c·ªông s·∫£n", "c·ªông ph·ªâ", 
        "t√†u c·ªông", "t√†u kh·ª±a", "b·ªçn ch·ªát"
    ]
    
    text_lower = text.lower().strip()
    return any(pattern in text_lower for pattern in special_patterns)

def remove_emojis(text):
    """X√≥a t·∫•t c·∫£ emoji kh·ªèi vƒÉn b·∫£n"""
    if not isinstance(text, str):
        return ""
        
    try:
        # S·ª≠ d·ª•ng regex ƒë·ªÉ x√≥a emoji
        emoji_pattern = re.compile(
            "["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F700-\U0001F77F"  # alchemical symbols
            u"\U0001F780-\U0001F7FF"  # Geometric Shapes
            u"\U0001F800-\U0001F8FF"  # Supplemental Arrows-C
            u"\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs
            u"\U0001FA00-\U0001FA6F"  # Chess Symbols
            u"\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-A
            u"\U00002702-\U000027B0"  # Dingbats
            u"\U000024C2-\U0001F251" 
            "]+", flags=re.UNICODE
        )
        return emoji_pattern.sub(r'', text)
    except:
        # Fallback n·∫øu c√≥ l·ªói v·ªõi regex
        return text

def remove_vn_emoticons(text):
    """X√≥a c√°c icon c·∫£m x√∫c ki·ªÉu Vi·ªát Nam"""
    if not isinstance(text, str):
        return text
        
    # Danh s√°ch c√°c pattern icon c·∫ßn x√≥a
    vn_emoticon_patterns = [
        r':[)]+',  # Matches :)), :))), etc.
        r'=[)]+',  # Matches =)), =))), etc.
        r':\(\(',  # Matches :((
        r'=\(\(',  # Matches =((
        r':>+',    # Matches :>, :>>, etc.
        r':<+',    # Matches :<, :<<, etc.
        r':v+',    # Matches :v, :vv, etc.
        r':V+',    # Matches :V, :VV, etc.
        r'=\)+',   # Matches =), =)), etc.
        r'=\(+',   # Matches =(, =((, etc.
    ]
    
    # √Åp d·ª•ng c√°c pattern ƒë·ªÉ x√≥a icon
    for pattern in vn_emoticon_patterns:
        text = re.sub(pattern, '', text)
    
    return text

def minimal_clean(text):
    """
    Th·ª±c hi·ªán minimal cleaning cho text:
    1. Chu·∫©n h√≥a Unicode (UTF-8)
    2. Lo·∫°i b·ªè URL, tag, emoji v√† c√°c ch·ªâ b√°o ph·ªï bi·∫øn
    3. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    """
    if not isinstance(text, str):
        return ""
    
    # Chu·∫©n h√≥a Unicode
    text = unicodedata.normalize('NFC', text)
    
    # Lo·∫°i b·ªè URLs - c·∫£i thi·ªán ƒë·ªÉ b·∫Øt c·∫£ domain tr∆°n nh∆∞ facebook.com
    text = re.sub(r'https?://\S+|www\.\S+|\S+\.(com|org|net|co|vn|io)(/\S*)?', '', text)
    
    # Lo·∫°i b·ªè c√°c MEDIA+N.GIPHY.COM, VD: MEDIA1.GIPHY.COM, MEDIA2.GIPHY.COM
    text = re.sub(r'media\d*\.giphy\.com', '', text)

    # Lo·∫°i b·ªè HTML tags
    text = re.sub(r'<[^>]+>', '', text)
    
    # Lo·∫°i b·ªè mentions @username v√† c√°c tham chi·∫øu m·∫°ng x√£ h·ªôi
    text = re.sub(r'@[\w\._]+', '', text)
    text = re.sub(r'\(\s*ig\s+[\w\._]+\s*\)', '', text)  # (ig username)
    text = re.sub(r'\(\s*instagram\s+[\w\._]+\s*\)', '', text)  # (instagram username)
    
    # Lo·∫°i b·ªè emoji
    text = remove_emojis(text)
    
    # Lo·∫°i b·ªè icon ki·ªÉu Vi·ªát Nam
    text = remove_vn_emoticons(text)
    
    # Lo·∫°i b·ªè c√°c ch·ªâ b√°o ph·ªï bi·∫øn
    patterns = [
        r'(?i)\[ƒê√£ ch·ªânh s·ª≠a\]',
        r'(?i)\(ƒê√£ ch·ªânh s·ª≠a\)',
        r'(?i)ƒê√£ ch·ªânh s·ª≠a',
        r'(?i)See Translation',
        r'(?i)Xem b·∫£n d·ªãch',
        r'(?i)See more',
        r'(?i)Xem th√™m',
        r'(?i)·∫®n b·ªõt',
        r'(?i)Xem √≠t h∆°n',
        r'(?i)D·ªãch',
        r'(?i)Translated',
        r'(?i)more',
        r'(?i)less'
    ]
    
    for pattern in patterns:
        text = re.sub(pattern, '', text)
    
    # Lo·∫°i b·ªè d·∫•u c√¢u ri√™ng l·∫ª ho·∫∑c c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ª©ng m·ªôt m√¨nh
    text = re.sub(r'(?<!\w)[\^\'\`\~\"\,\.]+(?!\w)', ' ', text)
    
    # L√†m s·∫°ch kho·∫£ng tr·∫Øng
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    return text

if __name__ == "__main__":
    try:
        print("="*60)
        print("STARTING DATA CLEANING PROCESS")
        print("="*60)
        
        args = parse_args()
        print(f"Arguments received:")
        print(f"  - Version: {args.version}")
        print(f"  - Source: {args.source}")
        print(f"  - Input file: {args.input_file}")
        print(f"  - Output file: {args.output_file}")
        print(f"  - Target: {args.target}")
        
        if not args.version:
            print("ERROR: Version argument is required!")
            sys.exit(1)
        
        print(f"\nStarting cleaning process...")
        result = clean_data(
            version=args.version,
            source=args.source,
            input_filename=args.input_file,
            target=args.target,
            output_filename=args.output_file
        )
        
        if result is None:
            print("ERROR: Cleaning process failed!")
            sys.exit(1)
        else:
            print("\n" + "="*60)
            print("DATA CLEANING COMPLETED SUCCESSFULLY!")
            print("="*60)
            
    except Exception as e:
        print(f"\nFATAL ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12387867,"sourceType":"datasetVersion","datasetId":7811352},{"sourceId":12387886,"sourceType":"datasetVersion","datasetId":7811369}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install libraries","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install emoji regex pandas unicodedata fasttext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:10:57.639221Z","iopub.execute_input":"2025-07-07T09:10:57.639479Z","iopub.status.idle":"2025-07-07T09:11:00.255106Z","shell.execute_reply.started":"2025-07-07T09:10:57.639457Z","shell.execute_reply":"2025-07-07T09:11:00.254345Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport regex\nimport emoji\nimport unicodedata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.256785Z","iopub.execute_input":"2025-07-07T09:11:00.257033Z","iopub.status.idle":"2025-07-07T09:11:00.569326Z","shell.execute_reply.started":"2025-07-07T09:11:00.257011Z","shell.execute_reply":"2025-07-07T09:11:00.568763Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Đọc file CSV\ndf = pd.read_csv(\"/kaggle/input/data-extracted/dataset_extracted.csv\")\n\nprint(\"=== Head ===\")\nprint(df.head())\n\nprint(\"\\n=== Shape ===\")\nprint(df.shape)\n\nprint(\"\\n=== Info ===\")\nprint(df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.569987Z","iopub.execute_input":"2025-07-07T09:11:00.570356Z","iopub.status.idle":"2025-07-07T09:11:00.809203Z","shell.execute_reply.started":"2025-07-07T09:11:00.570339Z","shell.execute_reply":"2025-07-07T09:11:00.808484Z"}},"outputs":[{"name":"stdout","text":"=== Head ===\n                                             summary  \\\n0  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n1  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n2  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n3  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n4  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n\n                                             comment            label  \n0  luận điệu của bọn phản động, sỏ lá, 3/// viết ...  KHONG_PHAN_DONG  \n1  vậy ông bảo đại, ông diệm, ông thiệu là đảng v...  KHONG_PHAN_DONG  \n2                              muôn đời của đám 3///  KHONG_PHAN_DONG  \n3  già rồi mà đần vậy cháu ? cộng sản đánh mỹ, đá...  KHONG_PHAN_DONG  \n4  đúng là 3/// xỏ lá, bác hồ mất nên các bác khó...  KHONG_PHAN_DONG  \n\n=== Shape ===\n(11827, 3)\n\n=== Info ===\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 11827 entries, 0 to 11826\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   summary  11827 non-null  object\n 1   comment  11827 non-null  object\n 2   label    11827 non-null  object\ndtypes: object(3)\nmemory usage: 277.3+ KB\nNone\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"selected_df = df[['summary', 'comment', 'label']]\nprint(selected_df.head())\n\nprint(\"\\n=== Shape ===\")\nprint(selected_df.shape)\n\nprint(\"\\n=== Info ===\")\nprint(selected_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.810016Z","iopub.execute_input":"2025-07-07T09:11:00.810308Z","iopub.status.idle":"2025-07-07T09:11:00.823600Z","shell.execute_reply.started":"2025-07-07T09:11:00.810291Z","shell.execute_reply":"2025-07-07T09:11:00.823051Z"}},"outputs":[{"name":"stdout","text":"                                             summary  \\\n0  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n1  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n2  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n3  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n4  1. Nội dung sơ lược: Bài viết chỉ trích Phạm V...   \n\n                                             comment            label  \n0  luận điệu của bọn phản động, sỏ lá, 3/// viết ...  KHONG_PHAN_DONG  \n1  vậy ông bảo đại, ông diệm, ông thiệu là đảng v...  KHONG_PHAN_DONG  \n2                              muôn đời của đám 3///  KHONG_PHAN_DONG  \n3  già rồi mà đần vậy cháu ? cộng sản đánh mỹ, đá...  KHONG_PHAN_DONG  \n4  đúng là 3/// xỏ lá, bác hồ mất nên các bác khó...  KHONG_PHAN_DONG  \n\n=== Shape ===\n(11827, 3)\n\n=== Info ===\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 11827 entries, 0 to 11826\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype \n---  ------   --------------  ----- \n 0   summary  11827 non-null  object\n 1   comment  11827 non-null  object\n 2   label    11827 non-null  object\ndtypes: object(3)\nmemory usage: 277.3+ KB\nNone\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 1.Normalize Unicode (NFC) + lowercase","metadata":{}},{"cell_type":"code","source":"import unicodedata\ndef normalize_unicode_lower(text):\n    text = unicodedata.normalize('NFC', text)\n    return text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.825485Z","iopub.execute_input":"2025-07-07T09:11:00.825718Z","iopub.status.idle":"2025-07-07T09:11:00.839196Z","shell.execute_reply.started":"2025-07-07T09:11:00.825701Z","shell.execute_reply":"2025-07-07T09:11:00.838548Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## 2.Remove Emoji, links/HTML/mentions/hashtags/UI indicators","metadata":{}},{"cell_type":"code","source":"import re\nimport emoji\nimport unicodedata\n\n# Cải thiện các pattern emoticon từ minimal_cleaning.py\nEMOTICON_PATTERNS = [\n    r\":\\)+\",       # :), :)), :))), ...\n    r\":\\(+\",       # :(, :((, ...\n    r\":v+\",        # :v, :vvv, ...\n    r\":V+\",        # :V, :VV, ... (thêm từ minimal_cleaning.py)\n    r\"=+\\)+\",      # =), =)), ...\n    r\"=+\\(+\",      # =(, =((, ...\n    r\":d+\",        # :d, :dd\n    r\":p+\",        # :p, :pp\n    r\"<3+\",        # <3<3<3\n    r\"=+\\]+\",      # =], =]], =]]], ...\n    r\"=+\\[+\",      # =[, =[[, =[[[ ...\n    r\":>+\",        # :>, :>>, ... (thêm từ minimal_cleaning.py)\n    r\":<+\",        # :<, :<<, ... (thêm từ minimal_cleaning.py)\n    r\":\\(\\(\",      # :(( (thêm từ minimal_cleaning.py)\n    r\"=\\(\\(\",      # =(( (thêm từ minimal_cleaning.py)\n]\n\nEMOTICON_REGEX = re.compile(\"|\".join(EMOTICON_PATTERNS), re.IGNORECASE)\n\n# Thêm EMOJI_REGEX từ minimal_cleaning.py để trường hợp không có thư viện emoji\nEMOJI_REGEX = re.compile(\n    \"[\"\n    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n    u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes\n    u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n    u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n    u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n    u\"\\U00002702-\\U000027B0\"  # Dingbats\n    u\"\\U000024C2-\\U0001F251\" \n    \"]+\", flags=re.UNICODE\n)\n\ndef remove_emoji_emoticon(text):\n    \"\"\"\n    Xóa emoji và emoticon từ văn bản.\n    Sử dụng thư viện emoji hoặc fallback sang regex nếu thư viện không hoạt động.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Xóa emoji bằng thư viện emoji\n    try:\n        text = emoji.replace_emoji(text, replace=\" \")\n    except:\n        # Fallback nếu thư viện emoji gặp lỗi\n        text = EMOJI_REGEX.sub(\" \", text)\n    \n    # Xóa emoticon bằng regex\n    text = EMOTICON_REGEX.sub(\" \", text)\n    \n    # Loại bỏ khoảng trắng dư thừa\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text\n\n# Regex\nURL_REGEX = re.compile(r'https?://\\S+|www\\.\\S+|\\S+\\.(com|org|net|co|vn|io)(/\\S*)?')\nHTML_REGEX = re.compile(r\"<[^>]+>\")\nMENTION_REGEX = re.compile(r\"@[\\w\\._]+\")  # Cải thiện để bắt dấu . và _\nHASHTAG_REGEX = re.compile(r\"#\\w+\")\nUI_INDICATORS = [\n    \"đã chỉnh sửa\", \"[đã chỉnh sửa]\", \"(đã chỉnh sửa)\",\n    \"see more\", \"xem thêm\", \"see translation\", \"xem bản dịch\",\n    \"ẩn bớt\", \"xem ít hơn\", \"dịch\", \"translated\", \"more\", \"less\",\n    \"see more reactions\"\n]\n\ndef remove_html_url_mention_hashtag(text):\n    \"\"\"\n    Loại bỏ URL, HTML tags, mentions, hashtags và các chỉ báo giao diện.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Chuẩn hóa Unicode (từ minimal_cleaning.py)\n    text = unicodedata.normalize('NFC', text)\n    \n    # Xóa URL\n    text = URL_REGEX.sub(\" \", text)\n    \n    # Xóa HTML tags\n    text = HTML_REGEX.sub(\" \", text)\n    \n    # Xóa mentions và hashtags\n    text = MENTION_REGEX.sub(\" \", text)\n    text = HASHTAG_REGEX.sub(\" \", text)\n    \n    # Xóa các chỉ báo giao diện (case insensitive)\n    for ind in UI_INDICATORS:\n        text = re.sub(r'(?i)' + re.escape(ind), \" \", text)\n    \n    # Loại bỏ dấu câu riêng lẻ\n    text = re.sub(r'(?<!\\w)[\\^\\'\\`\\~\\\"\\,\\.]+(?!\\w)', ' ', text)\n    \n    # Làm sạch khoảng trắng dư thừa\n    text = re.sub(r'\\s+', ' ', text).strip()\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.839820Z","iopub.execute_input":"2025-07-07T09:11:00.840019Z","iopub.status.idle":"2025-07-07T09:11:00.855235Z","shell.execute_reply.started":"2025-07-07T09:11:00.840003Z","shell.execute_reply":"2025-07-07T09:11:00.854608Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## 3.Remove punctuation + strip extra spaces","metadata":{}},{"cell_type":"code","source":"ALLOWED_SPECIAL_CHARS = r\"+/\\|=&\\.'|\"\nVIET_CHARACTERS = (\n    r\"àáạảãâầấậẩẫăằắặẳẵ\"\n    r\"èéẹẻẽêềếệểễ\"\n    r\"ìíịỉĩ\"\n    r\"òóọỏõôồốộổỗơờớợởỡ\"\n    r\"ùúụủũưừứựửữ\"\n    r\"ỳýỵỷỹ\"\n    r\"đ\"\n)\nINVALID_CHAR_REGEX = regex.compile(\n    f\"[^{VIET_CHARACTERS}0-9a-zA-Z {ALLOWED_SPECIAL_CHARS}]+\",\n    flags=regex.IGNORECASE\n)\n\ndef remove_punctuation_keep_vietnamese(text):\n    if not isinstance(text, str):\n        return \"\"\n    return INVALID_CHAR_REGEX.sub(\" \", text)\n\ndef strip_extra_spaces(text):\n    if not isinstance(text, str):\n        return \"\"\n    return regex.sub(r\"\\s+\", \" \", text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.856347Z","iopub.execute_input":"2025-07-07T09:11:00.856737Z","iopub.status.idle":"2025-07-07T09:11:00.873588Z","shell.execute_reply.started":"2025-07-07T09:11:00.856713Z","shell.execute_reply":"2025-07-07T09:11:00.873045Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def remove_punctuation_completely(text):\n    \"\"\"Loại bỏ hoàn toàn dấu câu, chỉ giữ chữ, số và space\"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    # Chỉ giữ lại chữ Việt, số, chữ Latin và khoảng trắng\n    pattern = f\"[^{VIET_CHARACTERS}0-9a-zA-Z\\\\s]+\"\n    text = re.sub(pattern, \" \", text)\n    return re.sub(r'\\s+', ' ', text).strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.874342Z","iopub.execute_input":"2025-07-07T09:11:00.874535Z","iopub.status.idle":"2025-07-07T09:11:00.892944Z","shell.execute_reply.started":"2025-07-07T09:11:00.874518Z","shell.execute_reply":"2025-07-07T09:11:00.892276Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 4.Reduce elongated characters","metadata":{}},{"cell_type":"code","source":"import regex\n\ndef reduce_elongated(text):\n    if not isinstance(text, str):\n        return \"\"\n    # \\p{L} khớp mọi ký tự chữ Unicode (bao gồm cả tiếng Việt có dấu)\n    pattern = regex.compile(r\"([\\p{L}])\\1{2,}\", flags=regex.IGNORECASE)\n    return pattern.sub(r\"\\1\\1\", text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.893642Z","iopub.execute_input":"2025-07-07T09:11:00.893850Z","iopub.status.idle":"2025-07-07T09:11:00.909183Z","shell.execute_reply.started":"2025-07-07T09:11:00.893834Z","shell.execute_reply":"2025-07-07T09:11:00.908564Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 5. De-teencode, Decode Abbreviate ","metadata":{}},{"cell_type":"code","source":"import re\nimport uuid\nimport json\n\n# Load dictionary với error handling\ntry:\n    with open('/kaggle/input/d/taitran501/dictionary/text_normalization_map.json', encoding='utf-8') as f:\n        norm_dict = json.load(f)\n    print(f\"✅ Đã load dictionary với {len(norm_dict)} entries\")\nexcept FileNotFoundError:\n    print(\"❌ Không tìm thấy file dictionary\")\n    norm_dict = {}\nexcept Exception as e:\n    print(f\"❌ Lỗi khi load dictionary: {str(e)}\")\n    norm_dict = {}\n\ndef normalize_teencode_improved(text, dictionary):\n    \"\"\"\n    Cải thiện thuật toán normalize teencode với pattern matching tốt hơn\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n    \n    # Phân loại entries thành 3 nhóm để xử lý riêng biệt\n    pure_symbol_entries = []     # Chỉ ký tự đặc biệt: \"///\", \"+\", \"=\"\n    mixed_entries = []           # Có chữ/số + ký tự đặc biệt: \"3///\", \"v+\", \"việt+\"\n    word_entries = []            # Chỉ chữ/số: \"ba\", \"cs\", \"csvn\"\n    \n    for slang, standard in dictionary.items():\n        if re.fullmatch(r\"[^\\w\\s]+\", slang):\n            # Chỉ toàn ký tự đặc biệt\n            pure_symbol_entries.append((slang, standard))\n        elif re.search(r\"[^\\w\\s]\", slang):\n            # Có ký tự đặc biệt mixed với chữ/số\n            mixed_entries.append((slang, standard))\n        else:\n            # Chỉ chữ/số/space\n            word_entries.append((slang, standard))\n    \n    # Sắp xếp mỗi nhóm theo độ dài giảm dần\n    pure_symbol_entries = sorted(pure_symbol_entries, key=lambda x: -len(x[0]))\n    mixed_entries = sorted(mixed_entries, key=lambda x: -len(x[0]))\n    word_entries = sorted(word_entries, key=lambda x: -len(x[0]))\n    \n    # Xử lý theo thứ tự: Mixed → Pure Symbol → Word\n    processing_order = mixed_entries + pure_symbol_entries + word_entries\n    \n    temp_map = {}\n    \n    for slang, standard in processing_order:\n        # Tạo token tạm unique\n        temp_token = f\"__TMP_{uuid.uuid4().hex[:8]}__\"\n        \n        # Chọn pattern phù hợp cho từng loại entry\n        if slang in [entry[0] for entry in pure_symbol_entries]:\n            # Pure symbols: exact match, không cần word boundary\n            pattern = re.compile(re.escape(slang), flags=re.IGNORECASE)\n            \n        elif slang in [entry[0] for entry in mixed_entries]:\n            # Mixed entries: cần xử lý cẩn thận word boundary\n            if slang.startswith(tuple('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')):\n                # Bắt đầu bằng chữ/số: cần word boundary ở đầu\n                if slang.endswith(tuple('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')):\n                    # Kết thúc bằng chữ/số: cần word boundary ở cuối\n                    pattern = re.compile(rf\"\\b{re.escape(slang)}\\b\", flags=re.IGNORECASE)\n                else:\n                    # Kết thúc bằng ký tự đặc biệt: chỉ cần word boundary ở đầu\n                    pattern = re.compile(rf\"\\b{re.escape(slang)}(?!\\w)\", flags=re.IGNORECASE)\n            else:\n                # Bắt đầu bằng ký tự đặc biệt\n                if slang.endswith(tuple('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')):\n                    # Kết thúc bằng chữ/số: cần word boundary ở cuối\n                    pattern = re.compile(rf\"(?<!\\w){re.escape(slang)}\\b\", flags=re.IGNORECASE)\n                else:\n                    # Kết thúc bằng ký tự đặc biệt: không cần word boundary\n                    pattern = re.compile(re.escape(slang), flags=re.IGNORECASE)\n                    \n        else:\n            # Word entries: dùng word boundary thông thường\n            pattern = re.compile(rf\"\\b{re.escape(slang)}\\b\", flags=re.IGNORECASE)\n        \n        # Thực hiện replace\n        new_text, count = pattern.subn(temp_token, text)\n        if count > 0:\n            temp_map[temp_token] = standard\n            text = new_text  # Cập nhật text ngay để tránh conflict\n    \n    # Cuối cùng, thay token tạm bằng từ chuẩn\n    for token, standard in temp_map.items():\n        text = text.replace(token, standard)\n    \n    return text\n\n# Function để apply lên dataframe\ndef apply_improved_normalize(text):\n    \"\"\"Wrapper function để apply lên pandas DataFrame\"\"\"\n    return normalize_teencode_improved(text, norm_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.909784Z","iopub.execute_input":"2025-07-07T09:11:00.910009Z","iopub.status.idle":"2025-07-07T09:11:00.987625Z","shell.execute_reply.started":"2025-07-07T09:11:00.909988Z","shell.execute_reply":"2025-07-07T09:11:00.986883Z"}},"outputs":[{"name":"stdout","text":"✅ Đã load dictionary với 462 entries\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 6. Deduplication ","metadata":{}},{"cell_type":"code","source":"def deduplicate_comments(df, col):\n    before = len(df)\n    df_nodup = df.drop_duplicates(subset=[col]).reset_index(drop=True)\n    after = len(df_nodup)\n    return df_nodup","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:00.988508Z","iopub.execute_input":"2025-07-07T09:11:00.988998Z","iopub.status.idle":"2025-07-07T09:11:00.999879Z","shell.execute_reply.started":"2025-07-07T09:11:00.988973Z","shell.execute_reply":"2025-07-07T09:11:00.999374Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ===== MAIN PREPROCESSING PIPELINE =====\nselected_df = selected_df.copy()\n\n# Store original count\noriginal_count = len(selected_df)\nprint(f\"Starting with {original_count:,} comments\")\n\n# 1. Normalize Unicode (NFC) + lowercase\nprint(\"1️. Normalizing Unicode and converting to lowercase\")\nselected_df['comment_clean'] = selected_df['comment'].apply(normalize_unicode_lower)\n\n# 2. Loại bỏ emoji, HTML, URL, mentions, hashtags, UI indicators\nprint(\"2️. Removing emoji, HTML, URLs, mentions, hashtags\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_html_url_mention_hashtag)\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_emoji_emoticon)\n\n# 3. Xóa ký tự không phải tiếng Việt/tiếng Anh + làm sạch khoảng trắng\nprint(\"3️. Removing non-Vietnamese/English characters\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation_keep_vietnamese)\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# 4. Giảm lặp ký tự kéo dài\nprint(\"4️. Reducing elongated characters\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(reduce_elongated)\n\n# 5. De-teencode với thuật toán cải thiện\nprint(\"5️. Applying improved de-teencode normalization\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(apply_improved_normalize)\n\n# 6. Loại bỏ hoàn toàn dấu câu\nprint(\"6️. Removing all punctuation\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(remove_punctuation_completely)\n\n# 7. Làm sạch khoảng trắng cuối cùng\nprint(\"7️.Final space cleanup\")\nselected_df['comment_clean'] = selected_df['comment_clean'].apply(strip_extra_spaces)\n\n# 8. Loại bỏ các comment trống hoặc quá ngắn\nprint(\"8️.Filtering out empty or too short comments\")\nbefore_filter = len(selected_df)\nselected_df = selected_df[\n    (selected_df['comment_clean'].str.len() >= 2) & \n    (selected_df['comment_clean'].str.strip() != \"\") &\n    (selected_df['comment_clean'] != \"\")\n].reset_index(drop=True)\nafter_filter = len(selected_df)\nprint(f\"  → Removed {before_filter - after_filter:,} short/empty comments\")\n\n# 9. Loại bỏ dòng trùng lặp\nbefore_dedup = len(selected_df)\nselected_df = deduplicate_comments(selected_df, col='comment_clean')\nafter_dedup = len(selected_df)\nprint(f\"  → Removed {before_dedup - after_dedup:,} duplicate comments\")\n\n# ===== STATISTICS =====\nprint(\"\\n\" + \"=\"*40)\nprint(\"=\"*40)\nprint(f\"  • Original comments:      {original_count:,}\")\nprint(f\"  • After filtering:        {after_filter:,} ({after_filter/original_count*100:.1f}%)\")\nprint(f\"  • Final comments:         {after_dedup:,} ({after_dedup/original_count*100:.1f}%)\")\nprint(f\"  • Total reduction:        {original_count - after_dedup:,} comments ({(original_count - after_dedup)/original_count*100:.1f}%)\")\n\n# Label distribution\nprint(f\"\\n Label Distribution:\")\nlabel_counts = selected_df['label'].value_counts()\nfor label, count in label_counts.items():\n    percentage = count / len(selected_df) * 100\n    print(f\"  • {label:<20}: {count:,} ({percentage:.1f}%)\")\n\n# ===== RESULTS =====\nprint(f\"\\n results:\")\nprint(\"-\" * 30)\n\n# Create final export dataframe\nexport_df = selected_df[['summary', 'comment_clean', 'label']].copy()\n\n# Export to CSV\nexport_df.to_csv(\"/kaggle/working/final_preprocessed_output.csv\", index=False, encoding=\"utf-8\")\nprint(f\" Saved {len(export_df):,} preprocessed comments to: final_preprocessed_output.csv\")\n\nprint(f\" Final dataset: {len(export_df):,} clean comments\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T09:11:01.000669Z","iopub.execute_input":"2025-07-07T09:11:01.000996Z","iopub.status.idle":"2025-07-07T09:11:55.907999Z","shell.execute_reply.started":"2025-07-07T09:11:01.000979Z","shell.execute_reply":"2025-07-07T09:11:55.907389Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Starting with 11,827 comments\n1️. Normalizing Unicode and converting to lowercase\n2️. Removing emoji, HTML, URLs, mentions, hashtags\n3️. Removing non-Vietnamese/English characters\n4️. Reducing elongated characters\n5️. Applying improved de-teencode normalization\n6️. Removing all punctuation\n7️.Final space cleanup\n8️.Filtering out empty or too short comments\n  → Removed 0 short/empty comments\n  → Removed 141 duplicate comments\n\n========================================\n========================================\n  • Original comments:      11,827\n  • After filtering:        11,827 (100.0%)\n  • Final comments:         11,686 (98.8%)\n  • Total reduction:        141 comments (1.2%)\n\n Label Distribution:\n  • KHONG_LIEN_QUAN     : 6,414 (54.9%)\n  • KHONG_PHAN_DONG     : 4,001 (34.2%)\n  • PHAN_DONG           : 1,271 (10.9%)\n\n results:\n------------------------------\n Saved 11,686 preprocessed comments to: final_preprocessed_output.csv\n Final dataset: 11,686 clean comments\n","output_type":"stream"}],"execution_count":12}]}
import re
import pandas as pd
import time
import itertools
import os
import argparse
import sys
from pathlib import Path
from tqdm import tqdm
import unicodedata
import json
import uuid
import numpy as np
from datetime import datetime, timedelta
from collections import defaultdict

# ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n import
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
sys.path.insert(0, str(parent_dir))

# Th√™m th∆∞ m·ª•c cha v√†o path ƒë·ªÉ import config
from utils.file_utils import save_excel_file
import config

# Parse command line arguments
def parse_args():
    parser = argparse.ArgumentParser(description='Summarize posts and prepare dataset')
    parser.add_argument('--version', '-v', help='Version to process (e.g., v1, v2)')
    parser.add_argument('--source', '-s', 
                        choices=['platform_split', 'output', 'merge', 'raw'], 
                        help='Source folder to process files from')
    parser.add_argument('--file', '-f', help='Specific file to process')
    parser.add_argument('--all', '-a', action='store_true', 
                        help='Process all Excel files in the selected folder')
    return parser.parse_args()

# Try to import google-generativeai with error handling
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
    print("‚úÖ Google GenerativeAI imported successfully")
except ImportError as e:
    GENAI_AVAILABLE = False
    print(f"‚ùå Failed to import google.generativeai: {e}")
    print("üîß Please run: pip install --force-reinstall google-generativeai protobuf==4.25.3")
    exit(1)

# ---- API CONFIGURATION WITH RATE LIMITING ----
# Import API keys from centralized config
from config import get_api_keys
API_KEYS = get_api_keys()

# Rate limits cho free tier (conservative values)
RATE_LIMITS = {
    "gemini-2.0-flash": {
        "rpm": 15,      # Requests per minute
        "tpm": 1000000, # Tokens per minute  
        "rpd": 1500     # Requests per day
    },
    "gemini-2.5-flash-preview-05-20": {
        "rpm": 10,
        "tpm": 250000,
        "rpd": 500
    }
}

BATCH_SIZE = 3    # Gi·∫£m batch size ƒë·ªÉ tr√°nh l·ªói
MAX_TOKENS = 4000 # TƒÉng token limit cho prompt ph·ª©c t·∫°p h∆°n
RETRY_ATTEMPTS = 3

class APIKeyManager:
    def __init__(self, api_keys, model_name="gemini-2.0-flash"):
        self.api_keys = api_keys
        self.current_key_index = 0
        self.model_name = model_name
        self.limits = RATE_LIMITS.get(model_name, RATE_LIMITS["gemini-2.0-flash"])
        
        # Tracking cho m·ªói API key
        self.usage_tracking = {key: {
            "requests_today": 0,
            "requests_this_minute": 0,
            "last_request_time": None,
            "last_reset_time": datetime.now()
        } for key in api_keys}
        
        self.setup_genai()
    
    def setup_genai(self):
        """Setup Google Generative AI with current API key"""
        try:
            genai.configure(api_key=self.api_keys[self.current_key_index])
            # Test connection
            list(genai.list_models())
            print(f"‚úÖ API Key {self.current_key_index + 1} configured successfully")
        except Exception as e:
            print(f"‚ùå Failed to configure API Key {self.current_key_index + 1}: {e}")
            self.switch_api_key()
    
    def switch_api_key(self):
        """Switch to next available API key"""
        self.current_key_index = (self.current_key_index + 1) % len(self.api_keys)
        print(f"üîÑ Switching to API Key {self.current_key_index + 1}")
        self.setup_genai()
    
    def can_make_request(self):
        """Check if we can make a request with current API key"""
        current_key = self.api_keys[self.current_key_index]
        usage = self.usage_tracking[current_key]
        now = datetime.now()
        
        # Reset daily counter if needed
        if now.date() > usage["last_reset_time"].date():
            usage["requests_today"] = 0
            usage["last_reset_time"] = now
        
        # Reset minute counter if needed
        if usage["last_request_time"] and (now - usage["last_request_time"]).seconds >= 60:
            usage["requests_this_minute"] = 0
        
        # Check limits
        if usage["requests_today"] >= self.limits["rpd"]:
            print(f"‚ö†Ô∏è Daily limit reached for API Key {self.current_key_index + 1}")
            return False
        
        if usage["requests_this_minute"] >= self.limits["rpm"]:
            print(f"‚ö†Ô∏è Minute limit reached for API Key {self.current_key_index + 1}")
            return False
        
        return True
    
    def find_available_key(self):
        """Find an API key that can make requests"""
        original_index = self.current_key_index
        
        while True:
            if self.can_make_request():
                return True
            
            self.switch_api_key()
            
            # If we've tried all keys, return False
            if self.current_key_index == original_index:
                return False
    
    def record_request(self):
        """Record that a request was made"""
        current_key = self.api_keys[self.current_key_index]
        usage = self.usage_tracking[current_key]
        now = datetime.now()
        
        usage["requests_today"] += 1
        usage["requests_this_minute"] += 1
        usage["last_request_time"] = now
    
    def wait_if_needed(self):
        """Wait if we need to respect rate limits"""
        if not self.find_available_key():
            # All keys are rate limited, wait
            print("‚è≥ All API keys are rate limited. Waiting 60 seconds...")
            time.sleep(60)
            return self.find_available_key()
        return True
    
    def get_usage_stats(self):
        """Get usage statistics for all API keys"""
        stats = {}
        for i, key in enumerate(self.api_keys):
            usage = self.usage_tracking[key]
            stats[f"Key_{i+1}"] = {
                "requests_today": usage["requests_today"],
                "requests_this_minute": usage["requests_this_minute"],
                "daily_limit": self.limits["rpd"],
                "minute_limit": self.limits["rpm"]
            }
        return stats

# ---- IMPROVED PROMPT WITH KEY TERMS RECOGNITION ----
IMPROVED_PROMPT = """Tr∆∞·ªõc khi t√≥m t·∫Øt, h√£y nh·∫≠n di·ªán c√°c t·ª´ kh√≥a/bi·ªát ng·ªØ ch√≠nh tr·ªã sau trong vƒÉn b·∫£n:

1. T·ª´ kh√≥a √°m ch·ªâ phe ·ªßng h·ªô ƒê·∫£ng C·ªông s·∫£n:
   - "b√≤ ƒë·ªè": ng∆∞·ªùi ·ªßng h·ªô/tuy√™n truy·ªÅn cho ƒê·∫£ng C·ªông s·∫£n, th∆∞·ªùng mang t√≠nh ch·∫ø gi·ªÖu
   - "d∆∞ lu·∫≠n vi√™n": ng∆∞·ªùi ƒë∆∞·ª£c cho l√† ƒë∆∞·ª£c thu√™ ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng d∆∞ lu·∫≠n ·ªßng h·ªô ch√≠nh ph·ªß
   - "R√©t bun": ch∆°i ch·ªØ t·ª´ "Red Bull", d√πng ƒë·ªÉ ch·ªâ phe "ƒë·ªè" (C·ªông s·∫£n) m·ªôt c√°ch m·ªâa mai

2. T·ª´ kh√≥a mi·ªát th·ªã ch·ªâ Vi·ªát Nam hi·ªán t·∫°i/ƒê·∫£ng C·ªông s·∫£n:
   - "v·∫πm" / "x·ª© v·∫πm": t·ª´ mi·ªát th·ªã ch·ªâ Vi·ªát Nam C·ªông s·∫£n (t·ª´ "Vi·ªát c·ªông")
   - "ƒë·∫Ωng": t·ª´ l√≥ng ch·ªâ ƒê·∫£ng C·ªông s·∫£n Vi·ªát Nam
   - "Vi·ªát c·ªông"/"vc": t·ª´ ch·ªâ ng∆∞·ªùi c·ªông s·∫£n Vi·ªát Nam, mang t√≠nh ch·∫•t mi·ªát th·ªã
   - "+ S·∫£n": vi·∫øt t·∫Øt, l√≥ng c·ªßa "C·ªông s·∫£n"
   - "B√©c" / "b√©c h√π" / "hochominh": c√°ch g·ªçi H·ªì Ch√≠ Minh theo l·ªëi ch·∫ø nh·∫°o
   - "T√¥ th·ªã L√¢m B√≤": √°m ch·ªâ T·ªïng B√≠ th∆∞ T√¥ L√¢m
   - "c·ªông ho√† xu·ªëng h·ªë c·∫£ n√∫t": √°m ch·ªâ ch·∫ø ƒë·ªô hi·ªán t·∫°i

3. T·ª´ kh√≥a li√™n quan ƒë·∫øn phe qu·ªëc gia/Vi·ªát Nam C·ªông H√≤a:
   - "C·ªù v√†ng ba s·ªçc ƒë·ªè": bi·ªÉu t∆∞·ª£ng Vi·ªát Nam C·ªông h√≤a, th∆∞·ªùng g·∫Øn v·ªõi ng∆∞·ªùi Vi·ªát h·∫£i ngo·∫°i
   - "ba que" / "3 que": t·ª´ l√≥ng ƒë·ªÉ ch·ªâ ng∆∞·ªùi ·ªßng h·ªô Vi·ªát Nam C·ªông H√≤a, t·ª´ mi·ªát th·ªã
   - "VNCH": vi·∫øt t·∫Øt c·ªßa Vi·ªát Nam C·ªông H√≤a
   - "ƒêu c√†ng Cali": √°m ch·ªâ ng∆∞·ªùi Vi·ªát h·∫£i ngo·∫°i ·ªü California
   - "H·∫≠u du·ªá Vi·ªát Nam C·ªông H√≤a": c√°ch g·ªçi con ch√°u ng∆∞·ªùi mi·ªÅn Nam di c∆∞ sau 1975

4. C√°c t·ª´ kh√≥a ch√≠nh tr·ªã kh√°c c·∫ßn ch√∫ √Ω:
   - "y√™u l∆∞·ªõc": c√°ch vi·∫øt ch·∫ø gi·ªÖu t·ª´ "y√™u n∆∞·ªõc", √°m ch·ªâ vi·ªác l·ª£i d·ª•ng l√≤ng y√™u n∆∞·ªõc
   - "M·∫Ωo": c√°ch g·ªçi M·ªπ, th∆∞·ªùng mang t√≠nh ch√¢m bi·∫øm
   - "t·ªôc c·ªëi": t·ª´ mi·ªát th·ªã ƒë·ªÉ ch·ªâ ng∆∞·ªùi mi·ªÅn B·∫Øc
   - "ch·ªát": t·ª´ mi·ªát th·ªã ƒë·ªÉ ch·ªâ ng∆∞·ªùi Trung Qu·ªëc
   - "ƒê·∫•u t·ªë": h√†nh ƒë·ªông t·ªë gi√°c, c√¥ng k√≠ch c√¥ng khai m·ªôt ng∆∞·ªùi
   - "tay sai": √°m ch·ªâ ng∆∞·ªùi l√†m vi·ªác cho th·∫ø l·ª±c n∆∞·ªõc ngo√†i
   - "ph·∫£n ƒë·ªông": t·ª´ d√πng ƒë·ªÉ ch·ªâ nh·ªØng ng∆∞·ªùi ch·ªëng ƒë·ªëi ch√≠nh quy·ªÅn
   - "ƒê·∫£ng tr·ªã": √°m ch·ªâ h·ªá th·ªëng ch√≠nh tr·ªã m·ªôt ƒë·∫£ng c·∫ßm quy·ªÅn
   - "barwhere", "cani": t·ª´ l√≥ng mi·ªát th·ªã, bi·∫øn t·∫•u t·ª´ ti·∫øng Anh
   - "ng·ª•y": t·ª´ mi·ªát th·ªã ch·ªâ ch√≠nh quy·ªÅn Vi·ªát Nam C·ªông H√≤a v√† ng∆∞·ªùi ·ªßng h·ªô

B√¢y gi·ªù, h√£y t√≥m t·∫Øt n·ªôi dung tr√™n th√†nh 3 m·ª•c, y√™u c·∫ßu ng·∫Øn g·ªçn:

{text_entries}

Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON ch√≠nh x√°c nh∆∞ sau:
```json
{{
  "results": [
    {{
      "id": "id1",
      "summary": "1. N·ªôi dung s∆° l∆∞·ª£c: [t√≥m t·∫Øt ng·∫Øn g·ªçn]\\n2. V·∫•n ƒë·ªÅ: [v·∫•n ƒë·ªÅ ch√≠nh]\\n3. Ph·∫£n ƒë·ªông/tin gi·∫£: [c√≥/kh√¥ng v√† gi·∫£i th√≠ch ng·∫Øn]"
    }},
    {{
      "id": "id2", 
      "summary": "1. N·ªôi dung s∆° l∆∞·ª£c: [t√≥m t·∫Øt ng·∫Øn g·ªçn]\\n2. V·∫•n ƒë·ªÅ: [v·∫•n ƒë·ªÅ ch√≠nh]\\n3. Ph·∫£n ƒë·ªông/tin gi·∫£: [c√≥/kh√¥ng v√† gi·∫£i th√≠ch ng·∫Øn]"
    }}
  ]
}}
```

L∆∞u √Ω: 
1. Tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† theo ƒë√∫ng format v·ªõi 3 m·ª•c nh∆∞ y√™u c·∫ßu.
2. N·∫øu vƒÉn b·∫£n ch·ª©a c√°c t·ª´ kh√≥a/bi·ªát ng·ªØ ƒë√£ li·ªát k√™ ·ªü tr√™n, h√£y ƒë√°nh gi√° ƒë√∫ng t√≠nh ch·∫•t ch√≠nh tr·ªã c·ªßa n√≥.
3. ƒê√°nh gi√° "Ph·∫£n ƒë·ªông/tin gi·∫£" c·∫ßn d·ª±a tr√™n vi·ªác c√≥ s·ª≠ d·ª•ng ng√¥n ng·ªØ th√π gh√©t, k√≠ch ƒë·ªông chia r·∫Ω, xuy√™n t·∫°c hay kh√¥ng.
4. Ngay c·∫£ khi vƒÉn b·∫£n ng·∫Øn, h√£y ch√∫ √Ω ƒë·∫øn c√°c t·ª´ kh√≥a v√† bi·ªát ng·ªØ ƒë√£ li·ªát k√™ ƒë·ªÉ ƒë√°nh gi√° ƒë√∫ng."""

def check_environment():
    """Ki·ªÉm tra m√¥i tr∆∞·ªùng tr∆∞·ªõc khi ch·∫°y"""
    print("üîç CHECKING ENVIRONMENT")
    print("-" * 40)
    
    # Check packages
    try:
        import google.generativeai as genai
        print("‚úÖ google-generativeai: OK")
    except ImportError:
        print("‚ùå google-generativeai: FAILED")
        return False
    
    try:
        import google.protobuf
        print(f"‚úÖ protobuf version: {google.protobuf.__version__}")
    except ImportError:
        print("‚ùå protobuf: FAILED")
        return False
    
    # Test API connection
    try:
        genai.configure(api_key=API_KEYS[0])
        models = list(genai.list_models())
        print(f"‚úÖ API connection: OK ({len(models)} models available)")
        return True
    except Exception as e:
        print(f"‚ùå API connection: FAILED - {e}")
        return False

def get_source_folder(version, source_type):
    """Get source folder path based on type"""
    paths = config.get_version_paths(version)
    
    if source_type == 'platform_split':
        return paths['raw_dir'].parent / "platform_split"
    elif source_type == 'output':
        return paths['output_dir']
    elif source_type == 'merge':
        return paths['merge_dir']
    elif source_type == 'raw':
        return paths['raw_dir']
    else:
        raise ValueError(f"Unknown source type: {source_type}")

def find_excel_files(folder_path):
    """Find all Excel files in a folder"""
    excel_files = list(folder_path.glob("*.xlsx"))
    excel_files.extend(list(folder_path.glob("*.xls")))
    return excel_files

def choose_source_and_files(version):
    """Interactive function to choose source folder and files"""
    print(f"\nüìÅ CH·ªåN NGU·ªíN D·ªÆ LI·ªÜU (VERSION {version})")
    print("-" * 50)
    
    # List available source folders
    source_options = ['platform_split', 'output', 'merge', 'raw']
    paths = config.get_version_paths(version)
    
    print("C√°c folder c√≥ s·∫µn:")
    available_sources = []
    for i, source in enumerate(source_options):
        folder_path = get_source_folder(version, source)
        excel_files = find_excel_files(folder_path)
        if folder_path.exists() and excel_files:
            available_sources.append(source)
            print(f"  {len(available_sources)}. {source} ({len(excel_files)} file Excel)")
        else:
            print(f"     {source} (kh√¥ng c√≥ file ho·∫∑c kh√¥ng t·ªìn t·∫°i)")
    
    if not available_sources:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file Excel n√†o trong t·∫•t c·∫£ c√°c folder!")
        return None, []
    
    # Choose source folder
    while True:
        try:
            choice = input(f"\nCh·ªçn source folder (1-{len(available_sources)}): ").strip()
            source_idx = int(choice) - 1
            if 0 <= source_idx < len(available_sources):
                selected_source = available_sources[source_idx]
                break
            else:
                print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë!")
    
    # Get files from selected source
    source_folder = get_source_folder(version, selected_source)
    excel_files = find_excel_files(source_folder)
    
    print(f"\nüìÑ CH·ªåN FILE T·ª™ {selected_source.upper()}")
    print("-" * 50)
    print("C√°c file c√≥ s·∫µn:")
    for i, file in enumerate(excel_files):
        print(f"  {i+1}. {file.name}")
    
    print(f"  {len(excel_files)+1}. T·∫•t c·∫£ file ({len(excel_files)} files)")
    
    # Choose files
    while True:
        try:
            choice = input(f"\nCh·ªçn file (1-{len(excel_files)+1}, ho·∫∑c 'a' cho t·∫•t c·∫£): ").strip().lower()
            
            if choice == 'a' or choice == str(len(excel_files)+1):
                return selected_source, excel_files
            else:
                file_idx = int(choice) - 1
                if 0 <= file_idx < len(excel_files):
                    return selected_source, [excel_files[file_idx]]
                else:
                    print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
        except ValueError:
            print("Vui l√≤ng nh·∫≠p s·ªë ho·∫∑c 'a'!")

def estimate_tokens(text):
    """∆Ø·ªõc t√≠nh tokens (4 chars = 1 token for Vietnamese)"""
    if not isinstance(text, str):
        return 0
    return max(1, len(text) // 4)

def clean_text(text):
    """L√†m s·∫°ch text ƒë·ªÉ tr√°nh safety filters"""
    if not isinstance(text, str):
        return ""
    
    text = unicodedata.normalize('NFC', text.strip())
    
    # Remove URLs, emails, phone numbers
    import re
    text = re.sub(r'http[s]?://\S+', '[URL]', text)
    text = re.sub(r'\S+@\S+', '[EMAIL]', text)
    text = re.sub(r'\d{3}-?\d{3}-?\d{4}', '[PHONE]', text)
    
    # Replace problematic characters
    text = text.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
    
    # Remove excessive punctuation
    text = re.sub(r'[!@#$%^&*()]+', ' ', text)
    
    return text

def create_batch_prompt(post_batch):
    """T·∫°o prompt cho batch posts"""
    text_entries = []
    batch_ids = []
    
    for idx, post in enumerate(post_batch):
        post_id = f"id{idx+1}"
        batch_ids.append(post_id)
        clean_post = clean_text(post)
        
        # Truncate if too long
        if estimate_tokens(clean_post) > MAX_TOKENS // len(post_batch):
            clean_post = clean_post[:int(len(clean_post) * 0.75)] + "..."
        
        text_entries.append(f"VƒÉn b·∫£n {post_id}:\n\"{clean_post}\"")
    
    formatted_entries = "\n\n".join(text_entries)
    
    # C·∫≠p nh·∫≠t prompt v·ªõi h∆∞·ªõng d·∫´n JSON r√µ r√†ng h∆°n
    prompt_template = IMPROVED_PROMPT.replace("{text_entries}", formatted_entries)
    prompt_template += """

L∆ØU √ù QUAN TR·ªåNG V·ªÄ ƒê·ªäNH D·∫†NG JSON:
1. ƒê·∫£m b·∫£o JSON tr·∫£ v·ªÅ PH·∫¢I h·ª£p l·ªá 100%.
2. KH√îNG s·ª≠ d·ª•ng d·∫•u xu·ªëng d√≤ng th·ª±c t·∫ø trong chu·ªói JSON, thay v√†o ƒë√≥ s·ª≠ d·ª•ng '\\n'.
3. Escape t·∫•t c·∫£ d·∫•u ngo·∫∑c k√©p trong chu·ªói JSON v·ªõi '\\\"'.
4. M·ªói th·∫ª 'summary' ph·∫£i l√† m·ªôt chu·ªói li√™n t·ª•c, kh√¥ng c√≥ ng·∫Øt d√≤ng th·∫≠t.
5. ƒê·∫∑t ph·∫ßn t√≥m t·∫Øt trong m·ªôt chu·ªói duy nh·∫•t, ƒë·∫£m b·∫£o c√≥ d·∫•u ph·∫©y ƒë√∫ng c√°ch gi·ªØa c√°c ƒë·ªëi t∆∞·ª£ng.
6. S·ª≠ d·ª•ng ƒë√∫ng ƒë·ªãnh d·∫°ng 'id1', 'id2', v.v. nh∆∞ ƒë√£ cung c·∫•p trong vƒÉn b·∫£n.

JSON PH·∫¢I c√≥ c·∫•u tr√∫c ch√≠nh x√°c nh∆∞ sau:
```json
{
  "results": [
    {
      "id": "id1",
      "summary": "1. N·ªôi dung s∆° l∆∞·ª£c: [t√≥m t·∫Øt]\\n2. V·∫•n ƒë·ªÅ: [v·∫•n ƒë·ªÅ]\\n3. Ph·∫£n ƒë·ªông/tin gi·∫£: [c√≥/kh√¥ng v√† gi·∫£i th√≠ch]"
    },
    {
      "id": "id2",
      "summary": "1. N·ªôi dung s∆° l∆∞·ª£c: [t√≥m t·∫Øt]\\n2. V·∫•n ƒë·ªÅ: [v·∫•n ƒë·ªÅ]\\n3. Ph·∫£n ƒë·ªông/tin gi·∫£: [c√≥/kh√¥ng v√† gi·∫£i th√≠ch]"
    }
  ]
}
```"""
    
    return prompt_template, batch_ids

def process_batch(api_manager, post_batch, batch_index, total_batches):
    """X·ª≠ l√Ω m·ªôt batch posts v·ªõi API manager"""
    # Convert NumPy array to list if needed
    if isinstance(post_batch, np.ndarray):
        post_batch = post_batch.tolist()
    
    # Check if batch is empty using length
    if len(post_batch) == 0:
        return {}
    
    prompt, batch_ids = create_batch_prompt(post_batch)
    estimated_tokens = estimate_tokens(prompt)
    
    print(f"\nüì¶ Processing batch {batch_index+1}/{total_batches}")
    print(f"   Posts in batch: {len(post_batch)}")
    print(f"   Estimated tokens: {estimated_tokens}")
    
    # Check if batch is too large
    if estimated_tokens > MAX_TOKENS:
        print(f"‚ö†Ô∏è  Batch too large ({estimated_tokens} tokens > {MAX_TOKENS})!")
        if len(post_batch) <= 1:
            # If single post is too large, truncate it
            print("   Single post too large, truncating...")
            prompt, batch_ids = create_batch_prompt([post_batch[0][:int(len(post_batch[0])*0.5)] + "..."])
        else:
            # Split batch in half and process recursively
            print("   Splitting batch in half...")
            mid = len(post_batch) // 2
            results1 = process_batch(api_manager, post_batch[:mid], batch_index, total_batches)
            results2 = process_batch(api_manager, post_batch[mid:], batch_index, total_batches)
            results1.update(results2)
            return results1
    
    # Process batch with retries
    for attempt in range(RETRY_ATTEMPTS):
        try:
            # Wait for rate limit using API manager
            if not api_manager.wait_if_needed():
                print("‚ùå All API keys exhausted for today")
                return {}
            
            current_key = api_manager.api_keys[api_manager.current_key_index]
            print(f"  üîë Using API key: ...{current_key[-4:]}")
            
            response = genai.GenerativeModel(api_manager.model_name).generate_content(
                prompt,
                generation_config={
                    "temperature": 0.1,
                    "max_output_tokens": 2048
                },
                safety_settings={
                    'HATE': 'BLOCK_NONE',
                    'HARASSMENT': 'BLOCK_NONE', 
                    'SEXUAL': 'BLOCK_NONE',
                    'DANGEROUS': 'BLOCK_NONE'
                }
            )
            
            # Record the request
            api_manager.record_request()
            
            # Log token usage
            try:
                usage = response.usage_metadata
                prompt_tokens = usage.prompt_token_count
                output_tokens = getattr(usage, 'candidates_token_count', 0)
                print(f"  ‚úÖ Batch {batch_index+1}/{total_batches} | " 
                      f"In: {prompt_tokens} | Out: {output_tokens} tokens")
            except:
                print(f"  ‚úÖ Batch {batch_index+1}/{total_batches} | Token usage unavailable")
            
            # Check if response is blocked or empty
            if not response.candidates or not response.candidates[0].content.parts:
                finish_reason = response.candidates[0].finish_reason if response.candidates else "UNKNOWN"
                print(f"  ‚ö†Ô∏è Response blocked or empty. Finish reason: {finish_reason}")
                
                # Create fallback summaries for this batch
                fallback_summaries = {}
                for post in post_batch:
                    fallback_summaries[post] = "N·ªôi dung b·ªã ch·∫∑n b·ªüi AI safety filter"
                return fallback_summaries
    
            # Extract text from response
            response_text = response.text.strip()
            
            # Super resilient JSON parsing
            try:
                results_dict = super_resilient_json_parser(response_text)
                
                # Map results to post_batch
                summaries = {}
                for result in results_dict.get('results', []):
                    result_id = result.get('id')
                    if result_id in batch_ids:
                        post_idx = batch_ids.index(result_id)
                        if post_idx < len(post_batch):
                            summaries[post_batch[post_idx]] = result.get('summary', '')
                
                print(f"  ‚úÖ Successfully processed {len(summaries)}/{len(post_batch)} posts")
                return summaries
                
            except json.JSONDecodeError as e:
                print(f"  ‚ùå All JSON parsing methods failed: {e}")
                print(f"  üìë Dumping response for debugging (first 200 chars): {response_text[:200]}...")
                
                # Ultimate fallback - extract anything that looks like a summary with regex
                summaries = {}
                pattern = r'1\.\s*N·ªôi dung s∆° l∆∞·ª£c:(.*?)(?:(?:\n|\\n)2\.|$)'
                matches = re.findall(pattern, response_text, re.DOTALL)
                
                if matches:
                    print(f"  üîÑ Last resort: Found {len(matches)} potential summaries")
                    for i, match in enumerate(matches):
                        if i < len(post_batch):
                            # Try to rebuild a complete summary by looking for parts 2 and 3
                            summary_text = f"1. N·ªôi dung s∆° l∆∞·ª£c:{match.strip()}"
                            
                            # Look for part 2
                            part2_match = re.search(r'2\.\s*V·∫•n ƒë·ªÅ:(.*?)(?:(?:\n|\\n)3\.|$)', response_text, re.DOTALL)
                            if part2_match:
                                summary_text += f"\n2. V·∫•n ƒë·ªÅ:{part2_match.group(1).strip()}"
                            
                            # Look for part 3
                            part3_match = re.search(r'3\.\s*Ph·∫£n ƒë·ªông/tin gi·∫£:(.*?)(?:\n|\\n|$)', response_text, re.DOTALL)
                            if part3_match:
                                summary_text += f"\n3. Ph·∫£n ƒë·ªông/tin gi·∫£:{part3_match.group(1).strip()}"
                            
                            summaries[post_batch[i]] = summary_text
                    
                    if summaries:
                        print(f"  ‚úÖ Extracted {len(summaries)} summaries through final fallback")
                        return summaries
                
                # If absolutely nothing worked, create placeholder summaries
                print("  ‚ö†Ô∏è Using placeholder summaries as last resort")
                placeholders = {}
                for idx, post in enumerate(post_batch):
                    placeholders[post] = f"1. N·ªôi dung s∆° l∆∞·ª£c: [L·ªói JSON]\n2. V·∫•n ƒë·ªÅ: Kh√¥ng x√°c ƒë·ªãnh\n3. Ph·∫£n ƒë·ªông/tin gi·∫£: Kh√¥ng x√°c ƒë·ªãnh"
                return placeholders
                
        except Exception as e:
            error_str = str(e)
            print(f"  ‚ùå Attempt {attempt+1} failed: {error_str}")
            
            # Check for specific error types
            if "finish_reason" in error_str and "2" in error_str:
                print(f"  üö´ Content blocked by safety filter")
                # Return fallback immediately for safety blocks
                fallback_summaries = {}
                for post in post_batch:
                    fallback_summaries[post] = "N·ªôi dung b·ªã ch·∫∑n b·ªüi AI safety filter"
                return fallback_summaries
            elif "429" in error_str or "quota" in error_str.lower():
                print(f"  üîÑ Rate limit detected, switching API key...")
                api_manager.switch_api_key()
                time.sleep(2)
            elif attempt < RETRY_ATTEMPTS - 1:
                time.sleep(10)
    
    # Fallback if all attempts fail
    print("  ‚ùå All attempts failed for this batch")
    fallback_summaries = {}
    for post in post_batch:
        fallback_summaries[post] = "Kh√¥ng th·ªÉ t√≥m t·∫Øt sau nhi·ªÅu l·∫ßn th·ª≠"
    return fallback_summaries

def save_error_log(batch_index, response_text, error):
    """Save error log for debugging"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path("./error_logs")
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"batch_{batch_index}_{timestamp}.log"
    
    with open(log_file, "w", encoding="utf-8") as f:
        f.write(f"ERROR: {error}\n\n")
        f.write("RESPONSE TEXT:\n")
        f.write(response_text)
    
    print(f"  üìù Error log saved to {log_file}")

def clean_json_text(text):
    """Clean up JSON text from response"""
    if '```json' in text:
        json_start = text.find('```json') + 7
        json_end = text.find('```', json_start)
        if json_end != -1:
            text = text[json_start:json_end].strip()
        else:
            text = text[json_start:].strip()
    elif '{' in text and '}' in text:
        start = text.find('{')
        end = text.rfind('}') + 1
        text = text[start:end]
    
    text = text.replace('```', '').strip()
    return text

def fix_json_format(json_text):
    """Fix common JSON format issues, especially with newlines"""
    # Level 1: Basic fixes
    json_text = re.sub(r'([^\\])\\n', r'\1\\n', json_text)  # Fix unescaped newlines
    json_text = re.sub(r'([^\\])\\\"', r'\1\\"', json_text)  # Fix unescaped quotes
    json_text = re.sub(r'([^\\])"([^"]*)"([^"]*)"', r'\1"\2\\"\3"', json_text)  # Fix nested quotes
    json_text = re.sub(r'"}(\s*){', r'"},\1{', json_text)  # Fix missing commas between objects
    
    # Level 2: More aggressive fixes
    json_text = re.sub(r'\n', '', json_text)  # Remove actual newlines
    json_text = re.sub(r'\\+n', '\\n', json_text)  # Fix multiple backslashes before n
    json_text = re.sub(r'\\+\"', '\\"', json_text)  # Fix multiple backslashes before quotes
    
    # Level 3: Extreme fixes
    try:
        # Try parsing as is
        json.loads(json_text)
        return json_text
    except json.JSONDecodeError as e:
        error_message = str(e)
        
        if "Expecting ',' delimiter" in error_message:
            # Fix missing commas - find position and insert comma
            position = int(re.search(r'char (\d+)', error_message).group(1))
            fixed_text = json_text[:position] + "," + json_text[position:]
            return fixed_text
            
        elif "Expecting property name enclosed in double quotes" in error_message:
            # Fix missing quotes around property names
            return re.sub(r'(\s+)(\w+)(:)', r'\1"\2"\3', json_text)
            
        elif "Unterminated string" in error_message:
            # Fix unterminated strings - add missing quote
            position = int(re.search(r'char (\d+)', error_message).group(1))
            fixed_text = json_text[:position] + "\"" + json_text[position:]
            return fixed_text
    
    # Return the original if nothing worked
    return json_text

def super_resilient_json_parser(response_text):
    """Super resilient JSON parser with multiple fallback strategies"""
    # Strategy 1: Clean and try to parse directly
    json_text = clean_json_text(response_text)
    
    try:
        return json.loads(json_text)
    except json.JSONDecodeError:
        print("  ‚ö†Ô∏è Initial JSON parse failed, trying fixes...")
    
    # Strategy 2: Apply fixes and try again
    fixed_json = fix_json_format(json_text)
    
    try:
        return json.loads(fixed_json)
    except json.JSONDecodeError:
        print("  ‚ö†Ô∏è Fixed JSON still failed, trying more aggressive fixes...")
    
    # Strategy 3: Rebuild JSON entirely if there's a pattern
    summaries = {}
    results = []
    
    # Try extracting with regex
    id_pattern = r'"id"\s*:\s*"(id\d+)"'
    summary_pattern = r'"summary"\s*:\s*"([^"]*(?:\\.[^"]*)*)(?<!\\)"'
    
    id_matches = re.findall(id_pattern, response_text)
    summary_matches = re.findall(summary_pattern, response_text)
    
    if id_matches and summary_matches and len(id_matches) == len(summary_matches):
        print(f"  ‚ö†Ô∏è Rebuilding JSON from {len(id_matches)} matched patterns")
        
        for i in range(len(id_matches)):
            result_id = id_matches[i]
            summary = summary_matches[i].replace('\\n', '\n').replace('\\"', '"')
            results.append({"id": result_id, "summary": summary})
        
        return {"results": results}
    
    # Strategy 4: Super aggressive - just extract based on labels
    if "1. N·ªôi dung s∆° l∆∞·ª£c:" in response_text:
        print("  ‚ö†Ô∏è Last resort: Rebuilding from text patterns")
        
        # Extract all post sections
        post_sections = re.split(r'"id"\s*:\s*"id\d+"', response_text)
        
        if len(post_sections) > 1:
            for i, section in enumerate(post_sections[1:], 1):  # Skip first empty split
                result_id = f"id{i}"
                
                # Try to extract the three parts
                summary_match = re.search(r'1\.\s*N·ªôi dung s∆° l∆∞·ª£c:\s*(.*?)(?:2\.|$)', section, re.DOTALL)
                problem_match = re.search(r'2\.\s*V·∫•n ƒë·ªÅ:\s*(.*?)(?:3\.|$)', section, re.DOTALL)
                fake_match = re.search(r'3\.\s*Ph·∫£n ƒë·ªông/tin gi·∫£:\s*(.*?)(?:"|$)', section, re.DOTALL)
                
                summary_text = ""
                if summary_match:
                    summary_text += f"1. N·ªôi dung s∆° l∆∞·ª£c: {summary_match.group(1).strip()}\n"
                if problem_match:
                    summary_text += f"2. V·∫•n ƒë·ªÅ: {problem_match.group(1).strip()}\n"
                if fake_match:
                    summary_text += f"3. Ph·∫£n ƒë·ªông/tin gi·∫£: {fake_match.group(1).strip()}"
                
                if summary_text:
                    results.append({"id": result_id, "summary": summary_text.strip()})
        
        if results:
            return {"results": results }
    
    # If all else fails, throw an error that will trigger the regex fallback
    raise json.JSONDecodeError("Failed all parsing strategies", json_text, 0)

def check_required_columns(df):
    """Ki·ªÉm tra c√°c c·ªôt b·∫Øt bu·ªôc trong DataFrame"""
    required_columns = ['post_raw']  # Ch·ªâ c·∫ßn post_raw l√† b·∫Øt bu·ªôc
    missing_columns = [col for col in required_columns if col not in df.columns]
    
    if missing_columns:
        print(f"‚ùå Thi·∫øu c√°c c·ªôt b·∫Øt bu·ªôc: {', '.join(missing_columns)}")
        return False
    
    # Ki·ªÉm tra c√°c c·ªôt kh√°c v√† th√¥ng b√°o
    optional_columns = ['post_id', 'comment_id', 'comment_raw', 'created_date', 'platform']
    available_optional = [col for col in optional_columns if col in df.columns]
    missing_optional = [col for col in optional_columns if col not in df.columns]
    
    if available_optional:
        print(f"‚úÖ C√°c c·ªôt c√≥ s·∫µn: {', '.join(available_optional)}")
    if missing_optional:
        print(f"‚ö†Ô∏è  C√°c c·ªôt t√πy ch·ªçn thi·∫øu: {', '.join(missing_optional)} (s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông)")
    
    return True

def process_single_file(api_manager, input_file, version, model_name):
    """Process a single Excel file"""
    print(f"\nüîÑ X·ª≠ l√Ω file: {input_file.name}")
    print("-" * 50)
    
    # Load data
    try:
        df_original = pd.read_excel(input_file)
        print(f"üìä ƒê√£ ƒë·ªçc file v·ªõi {len(df_original)} d√≤ng")
        print(f"üìã C√°c c·ªôt hi·ªán c√≥: {list(df_original.columns)}")
        
        # Check for required columns
        if not check_required_columns(df_original):
            return None
            
        # Get unique posts
        post_column = 'post_raw'
        unique_posts = df_original[post_column].dropna().unique().tolist()
        print(f"üîç T√¨m th·∫•y {len(unique_posts)} b√†i post duy nh·∫•t ƒë·ªÉ t√≥m t·∫Øt")
    except Exception as e:
        print(f"‚ùå Failed to load data: {e}")
        return None
    
    # Estimate time
    num_batches = (len(unique_posts) + BATCH_SIZE - 1) // BATCH_SIZE
    rate_limit = 60 / api_manager.limits["rpm"]  # Calculate delay based on RPM
    estimated_minutes = num_batches * rate_limit / 60
    
    print(f"\nüìä Th√¥ng tin x·ª≠ l√Ω:")
    print(f"   T·ªïng s·ªë b·∫£n ghi: {len(df_original):,}")
    print(f"   S·ªë post duy nh·∫•t: {len(unique_posts):,}")
    print(f"   K√≠ch th∆∞·ªõc batch: {BATCH_SIZE} posts/request")
    print(f"   S·ªë l∆∞·ª£ng batch: {num_batches}")
    print(f"   Model: {model_name}")
    print(f"   Rate limit: {rate_limit:.1f}s/batch")
    print(f"   Th·ªùi gian ∆∞·ªõc t√≠nh: {estimated_minutes:.1f} ph√∫t")
    
    # Create batches - Explicitly convert to list
    batches = []
    for i in range(0, len(unique_posts), BATCH_SIZE):
        batches.append(unique_posts[i:i+BATCH_SIZE])
    
    print(f"\nüîÑ ƒêang x·ª≠ l√Ω {len(unique_posts)} b√†i vi·∫øt trong {len(batches)} batch...")
    
    # Dictionary to store all summaries
    all_summaries = {}
    
    # Text comparison content
    txt_content = [
        "=" * 80,
        f"GEMINI {model_name.upper()} - K·∫æT QU·∫¢ PH√ÇN T√çCH C·∫¢I TI·∫æN",
        f"File: {input_file.name}",
        f"T·∫°o v√†o: {pd.Timestamp.now()}",
        f"T·ªïng s·ªë post: {len(unique_posts)}",
        f"K√≠ch th∆∞·ªõc batch: {BATCH_SIZE}",
        f"Model: {model_name}",
        "=" * 80,
        ""
    ]
    
    start_time = time.time()
    
    # Process all batches
    for batch_idx, batch in enumerate(tqdm(batches, desc="X·ª≠ l√Ω batch")):
        batch_results = process_batch(api_manager, batch, batch_idx, len(batches))
        all_summaries.update(batch_results)
        
        # Add batch results to text comparison
        for post_idx, post in enumerate(batch):
            post_num = batch_idx * BATCH_SIZE + post_idx + 1
            summary_text = batch_results.get(post, "‚ùå Kh√¥ng th·ªÉ t√≥m t·∫Øt")
            
            txt_content.extend([
                f"POST {post_num}:",
                "-" * 50,
                "ORIGINAL:",
                str(post),
                "",
                "SUMMARY:",
                str(summary_text),
                "",
                "=" * 80,
                ""
            ])
    
    # Add summary column to original DataFrame
    df_output = df_original.copy()
    
    # Create mapping and add summary column
    for post in unique_posts:
        summary_text = all_summaries.get(post, '')
        mask = df_output[post_column] == post
        df_output.loc[mask, 'summary'] = summary_text

    # Make sure all required columns are present
    required_cols = ['post_id', 'post_raw', 'comment_id', 'comment_raw', 'created_date', 'platform']
    for col in required_cols:
        if col not in df_output.columns:
            if col == 'post_id':
                df_output[col] = df_output.index.map(lambda x: f"post_{x+1}")
            elif col == 'comment_id':
                df_output[col] = df_output.index.map(lambda x: f"comment_{x+1}")
            elif col == 'created_date':
                df_output[col] = pd.Timestamp.now().strftime("%d-%m-%Y")
            elif col == 'platform':
                # Detect platform from filename
                filename = input_file.name.lower()
                if 'facebook' in filename:
                    df_output[col] = "Facebook"
                elif 'youtube' in filename:
                    df_output[col] = "YouTube"
                elif 'reddit' in filename:
                    df_output[col] = "Reddit"
                elif 'tiktok' in filename:
                    df_output[col] = "TikTok"
                elif 'threads' in filename:
                    df_output[col] = "Threads"
                else:
                    df_output[col] = "Unknown"
            else:
                df_output[col] = ""

    # Reorder columns according to the desired format
    desired_order = ['post_id', 'post_raw', 'summary', 'comment_id', 'comment_raw', 'created_date', 'platform']

    # Add 'label' column if it exists
    if 'label' in df_output.columns:
        desired_order.append('label')

    # Only keep columns that actually exist in the DataFrame
    available_columns = [col for col in desired_order if col in df_output.columns]
    df_output = df_output[available_columns]
    
    # Generate output filename - use 'summarized' not 'analyzed'
    file_stem = input_file.stem
    if not file_stem.endswith('_summarized'):
        file_stem += '_summarized'
    
    output_file = config.get_path(version, "summarized", filename=f"{file_stem}.xlsx")
    txt_file = config.get_path(version, "summarized", filename=f"{file_stem}_comparison.txt")
    
    # Ensure output directories exist
    output_file.parent.mkdir(parents=True, exist_ok=True)
    txt_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Save results
    try:
        df_output.to_excel(output_file, index=False)
        print(f"‚úÖ ƒê√£ l∆∞u Excel: {output_file}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u Excel: {e}")
        return None
    
    try:
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_content))
        print(f"‚úÖ ƒê√£ l∆∞u TXT: {txt_file}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u TXT: {e}")
    
    # Stats for this file
    elapsed_time = time.time() - start_time
    success_count = sum(1 for summary in all_summaries.values() 
                       if isinstance(summary, str) and 
                       summary and 
                       "Kh√¥ng th·ªÉ" not in summary and
                       "JSON l·ªói" not in summary and
                       "b·ªã ch·∫∑n" not in summary)
    
    print(f"\nüìä K·∫øt qu·∫£ file {input_file.name}:")
    print(f"   ‚úÖ Th√†nh c√¥ng: {success_count}/{len(unique_posts)}")
    print(f"   ‚ùå Th·∫•t b·∫°i: {len(unique_posts) - success_count}/{len(unique_posts)}")
    print(f"   ‚è±Ô∏è  Th·ªùi gian x·ª≠ l√Ω: {elapsed_time/60:.2f} ph√∫t")
    
    return {
        'file': input_file.name,
        'success': success_count,
        'total': len(unique_posts),
        'time': elapsed_time
    }

def list_available_models():
    """List all available Gemini models"""
    try:
        genai.configure(api_key=API_KEYS[0])
        models = list(genai.list_models())
        
        print("\nüìã Available Gemini models:")
        gemini_models = []
        for model in models:
            if 'gemini' in model.name.lower() and 'generateContent' in model.supported_generation_methods:
                model_name = model.name.split('/')[-1]  # Extract model name
                gemini_models.append(model_name)
                print(f"  - {model_name}")
        
        return gemini_models
    except Exception as e:
        print(f"‚ùå Failed to list models: {e}")
        return []

def choose_model():
    """Interactive function to choose model"""
    print("\nü§ñ CH·ªåN MODEL")
    print("-" * 40)
    
    # First, try to list available models
    available_models = list_available_models()
    
    if available_models:
        print(f"\nC√°c model c√≥ s·∫µn:")
        for i, model in enumerate(available_models):
            print(f"  {i+1}. {model}")
        
        while True:
            try:
                choice = input(f"\nCh·ªçn model (1-{len(available_models)}): ").strip()
                model_idx = int(choice) - 1
                if 0 <= model_idx < len(available_models):
                    selected_model = available_models[model_idx]
                    print(f"‚úÖ ƒê√£ ch·ªçn: {selected_model}")
                    return selected_model
                else:
                    print("L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")
            except ValueError:
                print("Vui l√≤ng nh·∫≠p s·ªë!")
    else:
        # Fallback to predefined choices
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ l·∫•y danh s√°ch model, s·ª≠ d·ª•ng l·ª±a ch·ªçn m·∫∑c ƒë·ªãnh:")
        print("  1. gemini-2.0-flash (faster, recommended)")
        print("  2. gemini-2.5-flash-preview-05-20 (higher quality)")
        
        model_choice = input("Select model (1 or 2): ").strip()
        if model_choice == "2":
            return "gemini-2.5-flash-preview-05-20"
        else:
            return "gemini-2.0-flash"

def main(version, source_type=None, target_files=None, process_all=False):
    """Main function - Analyze posts with improved prompt"""
    # Check environment first
    if not check_environment():
        print("\nüö® Environment check failed! Please fix the issues above.")
        return
    
    print("üîß INITIALIZING API MANAGEMENT")
    print("-" * 40)
    
    # Choose model interactively
    model_name = choose_model()
    
    # Initialize API manager
    api_manager = APIKeyManager(API_KEYS, model_name)
    
    # Choose source and files if not provided
    if source_type is None or target_files is None:
        source_type, target_files = choose_source_and_files(version)
        if source_type is None:
            return
    
    print(f"\nüöÄ GEMINI {model_name.upper()} - IMPROVED CONTENT ANALYSIS")
    print("=" * 70)
    print(f"Version: {version}")
    print(f"Source: {source_type}")
    print(f"Files to process: {len(target_files)}")
    for file in target_files:
        print(f"  - {file.name}")
    
    print(f"\nüìù Ph√¢n t√≠ch theo 3 m·ª•c:")
    print(f"   1. N·ªôi dung s∆° l∆∞·ª£c")
    print(f"   2. V·∫•n ƒë·ªÅ")
    print(f"   3. Ph·∫£n ƒë·ªông/tin gi·∫£ (c√≥/kh√¥ng v√† gi·∫£i th√≠ch)")
    
    # Confirm to proceed
    proceed = input(f"\nü§î Ti·∫øp t·ª•c x·ª≠ l√Ω {len(target_files)} file(s)? (y/n): ").lower().strip()
    if proceed != 'y':
        print("‚ùå ƒê√£ h·ªßy")
        return
    
    # Process all files
    results = []
    total_start_time = time.time()
    
    for i, file in enumerate(target_files):
        print(f"\n{'='*70}")
        print(f"FILE {i+1}/{len(target_files)}: {file.name}")
        print(f"{'='*70}")
        
        result = process_single_file(api_manager, file, version, model_name)
        if result:
            results.append(result)
    
    # Final summary
    total_elapsed = time.time() - total_start_time
    final_stats = api_manager.get_usage_stats()
    
    print(f"\nüéâ HO√ÄN TH√ÄNH T·∫§T C·∫¢!")
    print("=" * 70)
    print(f"üìä T·ªïng k·∫øt:")
    print(f"   üìÅ ƒê√£ x·ª≠ l√Ω: {len(results)}/{len(target_files)} file(s)")
    print(f"   ‚è±Ô∏è  T·ªïng th·ªùi gian: {total_elapsed/60:.2f} ph√∫t")
    
    if results:
        total_success = sum(r['success'] for r in results)
        total_posts = sum(r['total'] for r in results)
        success_rate = (total_success / total_posts * 100) if total_posts > 0 else 0
        
        print(f"   ‚úÖ T·ªïng posts th√†nh c√¥ng: {total_success}/{total_posts}")
        print(f"   üìà T·ª∑ l·ªá th√†nh c√¥ng: {success_rate:.1f}%")
    
    print(f"\nüìà Final API Usage:")
    for key, stats in final_stats.items():
        print(f"   {key}: {stats['requests_today']}/{stats['daily_limit']} requests today")
    
    if results:
        output_dir = config.get_path(version, "summarized").parent
        print(f"\nüìÅ K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u trong: {output_dir}")
    
    print(f"\n‚úÖ Step 5 completed successfully.")
    print(f"\n‚ú® Processing complete!")

if __name__ == "__main__":
    args = parse_args()
    
    if args.version:
        version = args.version
    else:
        version = input("Enter version (e.g., v1, v2): ").strip()
    
    if not version:
        print("‚ùå Version is required!")
        exit(1)
    
    main(version, args.source, None, args.all)
import pandas as pd
import time
import itertools
import os
from tqdm import tqdm
import unicodedata
import json
import uuid
import numpy as np
from dotenv import load_dotenv

# Try to import google-generativeai with error handling
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
    print("âœ… Google GenerativeAI imported successfully")
except ImportError as e:
    GENAI_AVAILABLE = False
    print(f"âŒ Failed to import google.generativeai: {e}")
    print("ðŸ”§ Please run: pip install --force-reinstall google-generativeai protobuf==4.25.3")
    exit(1)

# Táº£i biáº¿n mÃ´i trÆ°á»ng tá»« file .env
load_dotenv()

# Láº¥y danh sÃ¡ch API keys tá»« biáº¿n mÃ´i trÆ°á»ng, tÃ¡ch báº±ng dáº¥u ";"
api_keys_str = os.getenv("GEMINI_API_KEYS", "")
API_KEYS = [key.strip() for key in api_keys_str.split(";") if key.strip()]

# Táº¡o iterator xoay vÃ²ng API key
key_cycle = itertools.cycle(API_KEYS)

# Kiá»ƒm tra náº¿u khÃ´ng cÃ³ key nÃ o thÃ¬ cáº£nh bÃ¡o
if not API_KEYS:
    print("âŒ No API keys loaded. Please set GEMINI_API_KEYS in .env.")
    exit(1)


MODEL = "gemini-2.0-flash"
RATE_LIMIT = 4.1  # 60s/15 requests = 4s + buffer
BATCH_SIZE = 5    # Number of posts per request
MAX_TOKENS = 3000 # Maximum tokens per request 
RETRY_ATTEMPTS = 3

# ---- OPTIMIZED PROMPT ----
OPTIMIZED_PROMPT = """TÃ³m táº¯t má»—i Ä‘oáº¡n vÄƒn báº£n sau thÃ nh 3 má»¥c:
1. Ná»™i dung sÆ¡ lÆ°á»£c
2. Váº¥n Ä‘á»
3. Pháº£n Ä‘á»™ng/tin giáº£ (cÃ³/khÃ´ng vÃ  giáº£i thÃ­ch ngáº¯n)

{text_entries}

Tráº£ lá»i theo Ä‘á»‹nh dáº¡ng JSON chÃ­nh xÃ¡c nhÆ° sau, vá»›i má»—i Ä‘oáº¡n vÄƒn báº£n cÃ³ ID riÃªng:
```json
{{
  "results": [
    {{
      "id": "id1",
      "summary": {{
        "ná»™i_dung": "tÃ³m táº¯t ná»™i dung...",
        "váº¥n_Ä‘á»": "váº¥n Ä‘á» muá»‘n trÃ¬nh bÃ y...",
        "pháº£n_Ä‘á»™ng": "cÃ³/khÃ´ng vÃ  giáº£i thÃ­ch ngáº¯n..."
      }}
    }},
    {{
      "id": "id2",
      "summary": {{
        "ná»™i_dung": "tÃ³m táº¯t ná»™i dung...",
        "váº¥n_Ä‘á»": "váº¥n Ä‘á» muá»‘n trÃ¬nh bÃ y...",
        "pháº£n_Ä‘á»™ng": "cÃ³/khÃ´ng vÃ  giáº£i thÃ­ch ngáº¯n..."
      }}
    }}
  ]
}}
```"""

def check_environment():
    """Kiá»ƒm tra mÃ´i trÆ°á»ng trÆ°á»›c khi cháº¡y"""
    print("ðŸ” CHECKING ENVIRONMENT")
    print("-" * 40)
    
    # Check packages
    try:
        import google.generativeai as genai
        print("âœ… google-generativeai: OK")
    except ImportError:
        print("âŒ google-generativeai: FAILED")
        return False
    
    try:
        import google.protobuf
        print(f"âœ… protobuf version: {google.protobuf.__version__}")
    except ImportError:
        print("âŒ protobuf: FAILED")
        return False
    
    # Test API connection
    try:
        genai.configure(api_key=API_KEYS[0])
        models = list(genai.list_models())
        print(f"âœ… API connection: OK ({len(models)} models available)")
        return True
    except Exception as e:
        print(f"âŒ API connection: FAILED - {e}")
        return False

def set_next_key():
    """Chuyá»ƒn sang API key tiáº¿p theo"""
    api_key = next(key_cycle)
    genai.configure(api_key=api_key)
    return api_key

def estimate_tokens(text):
    """Æ¯á»›c tÃ­nh tokens (4 chars = 1 token for Vietnamese)"""
    if not isinstance(text, str):
        return 0
    return max(1, len(text) // 4)

def clean_text(text):
    """LÃ m sáº¡ch text"""
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize('NFC', text.strip())
    # Replace problematic characters
    text = text.replace('"', "'").replace('\n', ' ').replace('\r', ' ')
    return text

def create_batch_prompt(post_batch):
    """Táº¡o prompt cho batch posts"""
    text_entries = []
    batch_ids = []
    
    for idx, post in enumerate(post_batch):
        post_id = f"id{idx+1}"
        batch_ids.append(post_id)
        clean_post = clean_text(post)
        
        # Truncate if too long
        if estimate_tokens(clean_post) > MAX_TOKENS // len(post_batch):
            clean_post = clean_post[:int(len(clean_post) * 0.75)] + "..."
        
        text_entries.append(f"VÄƒn báº£n {post_id}:\n\"{clean_post}\"")
    
    formatted_entries = "\n\n".join(text_entries)
    prompt = OPTIMIZED_PROMPT.format(text_entries=formatted_entries)
    
    return prompt, batch_ids

def process_batch(post_batch, batch_index, total_batches):
    """Xá»­ lÃ½ má»™t batch posts"""
    # Convert NumPy array to list if needed
    if isinstance(post_batch, np.ndarray):
        post_batch = post_batch.tolist()
    
    # Check if batch is empty using length
    if len(post_batch) == 0:
        return {}
    
    prompt, batch_ids = create_batch_prompt(post_batch)
    estimated_tokens = estimate_tokens(prompt)
    
    print(f"\nðŸ“¦ Processing batch {batch_index+1}/{total_batches}")
    print(f"   Posts in batch: {len(post_batch)}")
    print(f"   Estimated tokens: {estimated_tokens}")
    
    # Check if batch is too large
    if estimated_tokens > MAX_TOKENS:
        print(f"âš ï¸  Batch too large ({estimated_tokens} tokens > {MAX_TOKENS})!")
        if len(post_batch) <= 1:
            # If single post is too large, truncate it
            print("   Single post too large, truncating...")
            prompt, batch_ids = create_batch_prompt([post_batch[0][:int(len(post_batch[0])*0.5)] + "..."])
        else:
            # Split batch in half and process recursively
            print("   Splitting batch in half...")
            mid = len(post_batch) // 2
            results1 = process_batch(post_batch[:mid], batch_index, total_batches)
            results2 = process_batch(post_batch[mid:], batch_index, total_batches)
            results1.update(results2)
            return results1
    
    # Process batch with retries
    for attempt in range(RETRY_ATTEMPTS):
        try:
            print(f"  â³ Waiting {RATE_LIMIT}s to respect rate limit...")
            time.sleep(RATE_LIMIT)
            
            current_key = set_next_key()
            print(f"  ðŸ”‘ Using API key: ...{current_key[-4:]}")
            
            response = genai.GenerativeModel(MODEL).generate_content(
                prompt,
                generation_config={
                    "temperature": 0.2,
                    "max_output_tokens": 1024
                }
            )
            
            # Log token usage
            try:
                usage = response.usage_metadata
                prompt_tokens = usage.prompt_token_count
                output_tokens = getattr(usage, 'candidates_token_count', 0)
                print(f"  âœ… Batch {batch_index+1}/{total_batches} | " 
                      f"In: {prompt_tokens} | Out: {output_tokens} tokens")
            except:
                print(f"  âœ… Batch {batch_index+1}/{total_batches} | Token usage unavailable")
            
            # Extract JSON from response
            response_text = response.text.strip()
            
            # Find JSON part
            json_start = response_text.find('```json')
            json_end = response_text.rfind('```')
            
            if json_start != -1 and json_end != -1 and json_end > json_start:
                json_text = response_text[json_start+7:json_end].strip()
            else:
                json_text = response_text
            
            # Clean up JSON text
            json_text = json_text.replace('```', '').strip()
            
            try:
                results_dict = json.loads(json_text)
                
                # Map results to post_batch
                summaries = {}
                for result in results_dict.get('results', []):
                    result_id = result.get('id')
                    if result_id in batch_ids:
                        post_idx = batch_ids.index(result_id)
                        if post_idx < len(post_batch):
                            summaries[post_batch[post_idx]] = result.get('summary', {})
                
                print(f"  âœ… Successfully processed {len(summaries)}/{len(post_batch)} posts")
                return summaries
                
            except json.JSONDecodeError as e:
                print(f"  âš ï¸ JSON parse error: {e}")
                print(f"  Raw response: {response_text[:100]}...")
                
                # Fallback for each post in batch
                fallback_summaries = {}
                for idx, post in enumerate(post_batch):
                    fallback_summaries[post] = {
                        "ná»™i_dung": f"KhÃ´ng thá»ƒ phÃ¢n tÃ­ch JSON tá»« response. Raw: {response_text[:50]}...",
                        "váº¥n_Ä‘á»": "Lá»—i xá»­ lÃ½",
                        "pháº£n_Ä‘á»™ng": "KhÃ´ng xÃ¡c Ä‘á»‹nh do lá»—i xá»­ lÃ½"
                    }
                return fallback_summaries
                
        except Exception as e:
            error_str = str(e)
            print(f"  âŒ Attempt {attempt+1} failed: {error_str}")
            
            if "429" in error_str or "quota" in error_str.lower():
                wait_time = 60
                print(f"  ðŸ• Rate limit hit! Waiting {wait_time}s for quota reset...")
                time.sleep(wait_time)
            elif attempt < RETRY_ATTEMPTS - 1:
                time.sleep(10)
    
    # Fallback if all attempts fail
    print("  âŒ All attempts failed for this batch")
    fallback = {}
    for post in post_batch:
        fallback[post] = {
            "ná»™i_dung": "KhÃ´ng thá»ƒ tÃ³m táº¯t sau nhiá»u láº§n thá»­",
            "váº¥n_Ä‘á»": "Lá»—i API",
            "pháº£n_Ä‘á»™ng": "KhÃ´ng xÃ¡c Ä‘á»‹nh do lá»—i xá»­ lÃ½"
        }
    return fallback

def format_summary(summary_dict):
    """Format summary dictionary into readable text"""
    if not summary_dict:
        return "âŒ KhÃ´ng thá»ƒ tÃ³m táº¯t"
    
    noi_dung = summary_dict.get('ná»™i_dung', 'KhÃ´ng cÃ³ thÃ´ng tin')
    van_de = summary_dict.get('váº¥n_Ä‘á»', 'KhÃ´ng cÃ³ thÃ´ng tin')
    phan_dong = summary_dict.get('pháº£n_Ä‘á»™ng', 'KhÃ´ng cÃ³ thÃ´ng tin')
    
    return f"""1. Ná»™i dung sÆ¡ lÆ°á»£c: {noi_dung}

2. Váº¥n Ä‘á»: {van_de}

3. Pháº£n Ä‘á»™ng/tin giáº£: {phan_dong}"""

def main_batch():
    """Main function vá»›i batch processing"""
    # Check environment first
    if not check_environment():
        print("\nðŸš¨ Environment check failed! Please fix the issues above.")
        return
    
    input_file = r"C:\Users\trant\Documents\Test\scrape\preprocessing\merged_all_platform\test_clean\dataset\dataset.xlsx"
    output_file = r"C:\Users\trant\Documents\Test\scrape\preprocessing\merged_all_platform\test_clean\dataset\dataset_labeled.xlsx"
    txt_file = r"C:\Users\trant\Documents\Test\scrape\preprocessing\merged_all_platform\test_clean\dataset\posts_comparison.txt"
    
    print("\nðŸš€ GEMINI 2.0 FLASH - BATCH PROCESSING")
    print("=" * 60)
    
    # Check input file
    if not os.path.exists(input_file):
        print(f"âŒ Input file not found: {input_file}")
        return
    
    # Load data
    try:
        df_original = pd.read_excel(input_file)
        # Convert NumPy array to list for safety
        unique_posts = df_original['post'].unique().tolist()
    except Exception as e:
        print(f"âŒ Failed to load data: {e}")
        return
    
    # Estimate time
    num_batches = (len(unique_posts) + BATCH_SIZE - 1) // BATCH_SIZE
    estimated_minutes = num_batches * RATE_LIMIT / 60
    
    print(f"ðŸ“Š Dataset Info:")
    print(f"   Total records: {len(df_original):,}")
    print(f"   Unique posts: {len(unique_posts):,}")
    print(f"   Batch size: {BATCH_SIZE} posts per request")
    print(f"   Number of batches: {num_batches}")
    print(f"   Model: {MODEL}")
    print(f"   Rate limit: {RATE_LIMIT}s per batch")
    print(f"   Estimated time: {estimated_minutes:.1f} minutes")
    
    # Confirm to proceed
    proceed = input(f"\nðŸ¤” Proceed with batch processing? (y/n): ").lower().strip()
    if proceed != 'y':
        print("âŒ Cancelled")
        return
    
    # Create batches - Explicitly convert to list
    batches = []
    for i in range(0, len(unique_posts), BATCH_SIZE):
        batches.append(unique_posts[i:i+BATCH_SIZE])
    
    print(f"\nðŸ”„ Processing {len(unique_posts)} posts in {len(batches)} batches...")
    
    # Dictionary to store all summaries
    all_summaries = {}
    
    # Text comparison content
    txt_content = [
        "=" * 80,
        f"GEMINI {MODEL.upper()} - BATCH SUMMARIZATION RESULTS",
        f"Generated on: {pd.Timestamp.now()}",
        f"Total posts: {len(unique_posts)}",
        f"Batch size: {BATCH_SIZE}",
        "=" * 80,
        ""
    ]
    
    start_time = time.time()
    
    # Process all batches
    for batch_idx, batch in enumerate(tqdm(batches, desc="Processing batches")):
        batch_results = process_batch(batch, batch_idx, len(batches))
        all_summaries.update(batch_results)
        
        # Add batch results to text comparison
        for post_idx, post in enumerate(batch):
            post_num = batch_idx * BATCH_SIZE + post_idx + 1
            summary_dict = batch_results.get(post, {})
            formatted_summary = format_summary(summary_dict)
            
            txt_content.extend([
                f"POST {post_num}:",
                "-" * 50,
                "ORIGINAL:",
                str(post),
                "",
                "SUMMARY:",
                formatted_summary,
                "",
                "=" * 80,
                ""
            ])
    
    # Create DataFrame with summaries
    posts_df = pd.DataFrame({'post_original': list(all_summaries.keys())})
    posts_df['post_summary'] = posts_df['post_original'].apply(
        lambda x: format_summary(all_summaries.get(x, {}))
    )
    
    # Merge with original dataset
    # Create a mapping from post to summary
    summary_mapping = dict(zip(posts_df['post_original'], posts_df['post_summary']))
    
    # Add summary column to original DataFrame
    df_output = df_original.copy()
    df_output['summary'] = df_output['post'].map(summary_mapping)
    
    # Save results
    try:
        df_output.to_excel(output_file, index=False)
        print(f"âœ… Excel saved: {output_file}")
    except Exception as e:
        print(f"âŒ Failed to save Excel: {e}")
    
    try:
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(txt_content))
        print(f"âœ… TXT saved: {txt_file}")
    except Exception as e:
        print(f"âŒ Failed to save TXT: {e}")
    
    # Final stats
    elapsed_time = time.time() - start_time
    success_count = sum(1 for summary in all_summaries.values() if 'ná»™i_dung' in summary)
    
    print(f"\nðŸŽ‰ COMPLETED!")
    print(f"ðŸ“Š Results:")
    print(f"   âœ… Successful: {success_count}/{len(unique_posts)}")
    print(f"   âŒ Failed: {len(unique_posts) - success_count}/{len(unique_posts)}")
    print(f"   â±ï¸  Time taken: {elapsed_time/60:.2f} minutes")
    print(f"   ðŸ’° Requests saved: {len(unique_posts) - len(batches)} (~{100 - len(batches)*100/len(unique_posts):.1f}%)")

if __name__ == "__main__":
    main_batch()